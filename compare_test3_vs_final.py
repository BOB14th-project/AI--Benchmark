#!/usr/bin/env python3
"""
Test 3 vs Final ê²°ê³¼ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸

3ì°¨ í…ŒìŠ¤íŠ¸ì™€ ìµœì¢… ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ê°œì„  ì •ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import json
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import seaborn as sns

sns.set_style("whitegrid")


def calculate_metrics_from_sum(results):
    """ì „ì²´ TP, FP, FN í•©ì‚° í›„ ë©”íŠ¸ë¦­ ê³„ì‚° (ì—ëŸ¬ ì¼€ì´ìŠ¤ ì œì™¸)"""
    valid_results = [r for r in results if 'error' not in r]

    total_tp = sum(r.get('true_positives', 0) for r in valid_results)
    total_fp = sum(r.get('false_positives', 0) for r in valid_results)
    total_fn = sum(r.get('false_negatives', 0) for r in valid_results)

    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

    return {
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'tp': total_tp,
        'fp': total_fp,
        'fn': total_fn,
        'valid_count': len(valid_results),
        'error_count': len(results) - len(valid_results)
    }


def analyze_file(file_path):
    """íŒŒì¼ ë¶„ì„"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    model_name = data['benchmark_info']['test_models'][0]
    results = data['results']

    # RAG í¬í•¨/ì œì™¸ ë¶„ë¦¬
    rag_results = [r for r in results if r.get('with_rag', False)]
    no_rag_results = [r for r in results if not r.get('with_rag', False)]

    # ì—ì´ì „íŠ¸ íƒ€ì…ë³„ ë¶„ì„
    agent_types = sorted(list(set(r.get('agent_type', 'unknown') for r in results)))

    rag_metrics_by_agent = {}
    no_rag_metrics_by_agent = {}

    for agent_type in agent_types:
        rag_agent = [r for r in rag_results if r.get('agent_type') == agent_type]
        no_rag_agent = [r for r in no_rag_results if r.get('agent_type') == agent_type]

        rag_metrics_by_agent[agent_type] = calculate_metrics_from_sum(rag_agent)
        no_rag_metrics_by_agent[agent_type] = calculate_metrics_from_sum(no_rag_agent)

    # ì „ì²´ ë©”íŠ¸ë¦­
    overall_rag = calculate_metrics_from_sum(rag_results)
    overall_no_rag = calculate_metrics_from_sum(no_rag_results)

    return {
        'model_name': model_name,
        'agent_types': agent_types,
        'rag_by_agent': rag_metrics_by_agent,
        'no_rag_by_agent': no_rag_metrics_by_agent,
        'overall_rag': overall_rag,
        'overall_no_rag': overall_no_rag
    }


def compare_versions(test3_data, final_data):
    """ë²„ì „ ê°„ ë¹„êµ"""
    comparison = {
        'model_name': test3_data['model_name'],
        'improvements': {}
    }

    # ì „ì²´ ì„±ëŠ¥ ë¹„êµ
    for metric in ['f1_score', 'precision', 'recall']:
        test3_rag = test3_data['overall_rag'][metric]
        final_rag = final_data['overall_rag'][metric]
        test3_no_rag = test3_data['overall_no_rag'][metric]
        final_no_rag = final_data['overall_no_rag'][metric]

        improvement_rag = final_rag - test3_rag
        improvement_no_rag = final_no_rag - test3_no_rag
        improvement_pct_rag = (improvement_rag / test3_rag * 100) if test3_rag > 0 else 0
        improvement_pct_no_rag = (improvement_no_rag / test3_no_rag * 100) if test3_no_rag > 0 else 0

        comparison['improvements'][metric] = {
            'test3_rag': test3_rag,
            'final_rag': final_rag,
            'improvement_rag': improvement_rag,
            'improvement_pct_rag': improvement_pct_rag,
            'test3_no_rag': test3_no_rag,
            'final_no_rag': final_no_rag,
            'improvement_no_rag': improvement_no_rag,
            'improvement_pct_no_rag': improvement_pct_no_rag
        }

    # ì—ì´ì „íŠ¸ë³„ ë¹„êµ
    agent_comparison = {}
    for agent_type in test3_data['agent_types']:
        if agent_type in final_data['agent_types']:
            test3_f1 = test3_data['rag_by_agent'][agent_type]['f1_score']
            final_f1 = final_data['rag_by_agent'][agent_type]['f1_score']
            improvement = final_f1 - test3_f1
            improvement_pct = (improvement / test3_f1 * 100) if test3_f1 > 0 else 0

            agent_comparison[agent_type] = {
                'test3': test3_f1,
                'final': final_f1,
                'improvement': improvement,
                'improvement_pct': improvement_pct
            }

    comparison['agent_comparison'] = agent_comparison

    return comparison


def print_comparison(comparison):
    """ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
    model_name = comparison['model_name']

    print(f"\n{'='*100}")
    print(f"ğŸ“Š {model_name} - Test 3 vs Final ë¹„êµ")
    print(f"{'='*100}")

    # ì „ì²´ ì„±ëŠ¥ ë¹„êµ
    print(f"\nğŸ¯ ì „ì²´ ì„±ëŠ¥ ë¹„êµ (With RAG):")
    print(f"{'-'*100}")

    for metric in ['f1_score', 'precision', 'recall']:
        data = comparison['improvements'][metric]
        metric_name = metric.replace('_', ' ').title()

        print(f"\n{metric_name}:")
        print(f"  Test 3: {data['test3_rag']:.4f}")
        print(f"  Final:  {data['final_rag']:.4f}")

        arrow = "ğŸ“ˆ" if data['improvement_rag'] > 0 else "ğŸ“‰" if data['improvement_rag'] < 0 else "â¡ï¸"
        print(f"  {arrow} ë³€í™”: {data['improvement_rag']:+.4f} ({data['improvement_pct_rag']:+.1f}%)")

    print(f"\nğŸ¯ ì „ì²´ ì„±ëŠ¥ ë¹„êµ (Without RAG):")
    print(f"{'-'*100}")

    for metric in ['f1_score', 'precision', 'recall']:
        data = comparison['improvements'][metric]
        metric_name = metric.replace('_', ' ').title()

        print(f"\n{metric_name}:")
        print(f"  Test 3: {data['test3_no_rag']:.4f}")
        print(f"  Final:  {data['final_no_rag']:.4f}")

        arrow = "ğŸ“ˆ" if data['improvement_no_rag'] > 0 else "ğŸ“‰" if data['improvement_no_rag'] < 0 else "â¡ï¸"
        print(f"  {arrow} ë³€í™”: {data['improvement_no_rag']:+.4f} ({data['improvement_pct_no_rag']:+.1f}%)")

    # ì—ì´ì „íŠ¸ë³„ ë¹„êµ
    print(f"\nğŸ“Œ ì—ì´ì „íŠ¸ë³„ F1 Score ë³€í™” (With RAG):")
    print(f"{'-'*100}")

    agent_comp = comparison['agent_comparison']
    for agent_type in sorted(agent_comp.keys()):
        data = agent_comp[agent_type]
        arrow = "ğŸ“ˆ" if data['improvement'] > 0 else "ğŸ“‰" if data['improvement'] < 0 else "â¡ï¸"

        print(f"{agent_type:20s} | Test 3: {data['test3']:.4f} | Final: {data['final']:.4f} | "
              f"{arrow} {data['improvement']:+.4f} ({data['improvement_pct']:+.1f}%)")


def visualize_comparison(all_comparisons, output_dir):
    """ë¹„êµ ê²°ê³¼ ì‹œê°í™”"""

    # 1. ì „ì²´ F1 Score ë¹„êµ
    fig, axes = plt.subplots(2, 1, figsize=(14, 12))
    fig.suptitle('Test 3 vs Final - F1 Score Comparison', fontsize=16, fontweight='bold')

    # With RAG
    ax = axes[0]
    models = [comp['model_name'] for comp in all_comparisons]
    test3_rag = [comp['improvements']['f1_score']['test3_rag'] for comp in all_comparisons]
    final_rag = [comp['improvements']['f1_score']['final_rag'] for comp in all_comparisons]

    x = np.arange(len(models))
    width = 0.35

    bars1 = ax.bar(x - width/2, test3_rag, width, label='Test 3', color='#3498db', alpha=0.8)
    bars2 = ax.bar(x + width/2, final_rag, width, label='Final', color='#2ecc71', alpha=0.8)

    ax.set_ylabel('F1 Score', fontsize=13, fontweight='bold')
    ax.set_title('With RAG', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(models, fontsize=11)
    ax.legend(fontsize=12)
    ax.set_ylim([0, 0.5])
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.4f}',
                    ha='center', va='bottom', fontsize=10, fontweight='bold')

    # ê°œì„  í¼ì„¼íŠ¸ í‘œì‹œ
    for i, comp in enumerate(all_comparisons):
        improvement_pct = comp['improvements']['f1_score']['improvement_pct_rag']
        y_pos = max(test3_rag[i], final_rag[i]) + 0.02
        arrow = "â†‘" if improvement_pct > 0 else "â†“" if improvement_pct < 0 else "â†’"
        color = 'green' if improvement_pct > 0 else 'red' if improvement_pct < 0 else 'gray'
        ax.text(i, y_pos, f'{arrow}{abs(improvement_pct):.1f}%',
                ha='center', fontsize=10, fontweight='bold', color=color)

    # Without RAG
    ax = axes[1]
    test3_no_rag = [comp['improvements']['f1_score']['test3_no_rag'] for comp in all_comparisons]
    final_no_rag = [comp['improvements']['f1_score']['final_no_rag'] for comp in all_comparisons]

    bars1 = ax.bar(x - width/2, test3_no_rag, width, label='Test 3', color='#3498db', alpha=0.8)
    bars2 = ax.bar(x + width/2, final_no_rag, width, label='Final', color='#2ecc71', alpha=0.8)

    ax.set_ylabel('F1 Score', fontsize=13, fontweight='bold')
    ax.set_title('Without RAG', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(models, fontsize=11)
    ax.legend(fontsize=12)
    ax.set_ylim([0, 0.5])
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.4f}',
                    ha='center', va='bottom', fontsize=10, fontweight='bold')

    for i, comp in enumerate(all_comparisons):
        improvement_pct = comp['improvements']['f1_score']['improvement_pct_no_rag']
        y_pos = max(test3_no_rag[i], final_no_rag[i]) + 0.02
        arrow = "â†‘" if improvement_pct > 0 else "â†“" if improvement_pct < 0 else "â†’"
        color = 'green' if improvement_pct > 0 else 'red' if improvement_pct < 0 else 'gray'
        ax.text(i, y_pos, f'{arrow}{abs(improvement_pct):.1f}%',
                ha='center', fontsize=10, fontweight='bold', color=color)

    plt.tight_layout()
    output_file = output_dir / 'test3_vs_final_comparison.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"\nâœ… Saved: {output_file}")
    plt.close()

    # 2. ê°œì„ ìœ¨ ë¹„êµ
    fig, ax = plt.subplots(figsize=(12, 7))
    fig.suptitle('Improvement Rate: Final vs Test 3', fontsize=16, fontweight='bold')

    improvement_rag = [comp['improvements']['f1_score']['improvement_pct_rag'] for comp in all_comparisons]
    improvement_no_rag = [comp['improvements']['f1_score']['improvement_pct_no_rag'] for comp in all_comparisons]

    x = np.arange(len(models))
    width = 0.35

    bars1 = ax.bar(x - width/2, improvement_rag, width, label='With RAG', alpha=0.8)
    bars2 = ax.bar(x + width/2, improvement_no_rag, width, label='Without RAG', alpha=0.8)

    # ìƒ‰ìƒ ì„¤ì •
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            bar.set_color('#2ecc71' if height > 0 else '#e74c3c' if height < 0 else '#95a5a6')

    ax.set_ylabel('Improvement (%)', fontsize=13, fontweight='bold')
    ax.set_title('F1 Score Improvement Rate', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(models, fontsize=11)
    ax.legend(fontsize=12)
    ax.axhline(y=0, color='black', linestyle='-', linewidth=1)
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:+.1f}%',
                    ha='center', va='bottom' if height > 0 else 'top',
                    fontsize=11, fontweight='bold')

    plt.tight_layout()
    output_file = output_dir / 'test3_vs_final_improvement_rate.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {output_file}")
    plt.close()


def main():
    results_dir = Path('results')
    backup_dir = results_dir / 'backup'

    models = {
        'llama': {
            'test3': backup_dir / 'llama_test_3.json',
            'final': results_dir / 'llama_final.json'
        },
        'gemini': {
            'test3': backup_dir / 'gemini_test_3.json',
            'final': results_dir / 'gemini_final.json'
        },
        'gpt': {
            'test3': backup_dir / 'gpt_test_3.json',
            'final': results_dir / 'gpt_final.json'
        }
    }

    print(f"\n{'='*100}")
    print(f"ğŸ”¬ Test 3 vs Final ê²°ê³¼ ë¹„êµ ë¶„ì„")
    print(f"{'='*100}")

    all_comparisons = []

    for name, files in models.items():
        if not files['test3'].exists() or not files['final'].exists():
            print(f"âš ï¸  Missing files for {name}")
            continue

        print(f"\nì²˜ë¦¬ ì¤‘: {name}")

        test3_data = analyze_file(files['test3'])
        final_data = analyze_file(files['final'])

        comparison = compare_versions(test3_data, final_data)
        all_comparisons.append(comparison)

        print_comparison(comparison)

    # ì‹œê°í™”
    print(f"\n{'='*100}")
    print(f"ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
    print(f"{'='*100}")

    visualize_comparison(all_comparisons, results_dir)

    print(f"\n{'='*100}")
    print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"{'='*100}\n")


if __name__ == "__main__":
    main()
