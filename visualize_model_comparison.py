#!/usr/bin/env python3
"""
ëª¨ë¸ë³„ RAG ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸

llama, gemini, gpt test_3 íŒŒì¼ì—ì„œ With RAG ì„±ëŠ¥ë§Œ ë¹„êµí•©ë‹ˆë‹¤.
"""

import json
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path


def calculate_metrics_from_sum(results):
    """ì „ì²´ TP, FP, FN í•©ì‚° í›„ ë©”íŠ¸ë¦­ ê³„ì‚° (ì—ëŸ¬ ì¼€ì´ìŠ¤ ì œì™¸)"""
    # 'error' í”Œë˜ê·¸ê°€ ì—†ëŠ” ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
    valid_results = [r for r in results if 'error' not in r]

    total_tp = sum(r.get('true_positives', 0) for r in valid_results)
    total_fp = sum(r.get('false_positives', 0) for r in valid_results)
    total_fn = sum(r.get('false_negatives', 0) for r in valid_results)

    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

    return {
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'tp': total_tp,
        'fp': total_fp,
        'fn': total_fn,
        'valid_count': len(valid_results),
        'error_count': len(results) - len(valid_results)
    }


def visualize_model_comparison(result_files: list, output_file: Path):
    """3ê°œ ëª¨ë¸ì˜ With RAG ì„±ëŠ¥ ë¹„êµ"""

    # ê° ëª¨ë¸ì˜ ë°ì´í„° ë¡œë“œ
    models_data = {}

    for file_path in result_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # ëª¨ë¸ëª… ì¶”ì¶œ
        model_name = data['benchmark_info']['test_models'][0]

        # With RAG ê²°ê³¼ë§Œ í•„í„°ë§
        rag_results = [r for r in data['results'] if r.get('with_rag', False)]

        models_data[model_name] = rag_results

    # ëª¨ë“  ì—ì´ì „íŠ¸ íƒ€ì… ì¶”ì¶œ
    all_agent_types = set()
    for results in models_data.values():
        for r in results:
            all_agent_types.add(r.get('agent_type', 'unknown'))

    agent_types = sorted(list(all_agent_types))

    # ê° ëª¨ë¸ë³„, ì—ì´ì „íŠ¸ë³„ F1 Score ê³„ì‚°
    model_names = list(models_data.keys())
    f1_scores = {model: [] for model in model_names}

    for agent_type in agent_types:
        for model_name, results in models_data.items():
            agent_results = [r for r in results if r.get('agent_type') == agent_type]
            metrics = calculate_metrics_from_sum(agent_results)
            f1_scores[model_name].append(metrics['f1_score'])

    # Figure ìƒì„±
    fig, ax = plt.subplots(figsize=(12, 7))
    fig.suptitle('Model Comparison - F1 Score by Agent Type (With RAG Only)',
                 fontsize=15, fontweight='bold')

    # ë§‰ëŒ€ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    x = np.arange(len(agent_types))
    width = 0.25  # 3ê°œ ëª¨ë¸ì´ë¯€ë¡œ ì¢ê²Œ

    colors = {
        'llama3:8b': '#e74c3c',
        'gemini-2.0-flash': '#3498db',
        'gpt-4.1': '#2ecc71'
    }

    # ê° ëª¨ë¸ë³„ë¡œ ë§‰ëŒ€ ê·¸ë¦¬ê¸°
    bars_list = []
    for i, model_name in enumerate(model_names):
        offset = (i - 1) * width  # -1, 0, 1
        color = colors.get(model_name, f'C{i}')
        bars = ax.bar(x + offset, f1_scores[model_name], width,
                     label=model_name, color=color, alpha=0.8)
        bars_list.append(bars)

    # ì¶• ì„¤ì •
    ax.set_ylabel('F1 Score', fontsize=13, fontweight='bold')
    ax.set_xlabel('Agent Type', fontsize=13, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(agent_types, fontsize=11)
    ax.legend(fontsize=11, loc='upper right')
    ax.set_ylim([0, 1.0])
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
    for bars in bars_list:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.3f}',
                    ha='center', va='bottom', fontsize=9, fontweight='bold')

    # ë ˆì´ì•„ì›ƒ ì¡°ì •
    plt.tight_layout()

    # ì €ì¥
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ… Saved: {output_file}")

    plt.close()


def main():
    # ì…ë ¥ íŒŒì¼ë“¤
    results_dir = Path('results')
    result_files = [
        results_dir / 'llama_test_3.json',
        results_dir / 'gemini_test_3.json',
        results_dir / 'gpt_test_3.json'
    ]

    # ëª¨ë“  íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    for file_path in result_files:
        if not file_path.exists():
            print(f"âŒ File not found: {file_path}")
            return

    print(f"\n{'='*80}")
    print(f"ğŸ¨ ëª¨ë¸ ë¹„êµ ì‹œê°í™” ì‹œì‘")
    print(f"{'='*80}")
    print(f"ë¹„êµ ëª¨ë¸:")
    for file_path in result_files:
        print(f"  - {file_path.name}")
    print(f"{'='*80}\n")

    # ì¶œë ¥ íŒŒì¼
    output_file = results_dir / 'model_comparison_test3.png'

    # ì‹œê°í™” ìƒì„±
    visualize_model_comparison(result_files, output_file)

    print(f"\n{'='*80}")
    print(f"âœ… ì‹œê°í™” ì™„ë£Œ!")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
