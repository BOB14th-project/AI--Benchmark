# λ΅μ»¬ AI μ„λ²„ λ²¤μΉλ§ν¬ ν†µν•© κ°€μ΄λ“

μ΄ κ°€μ΄λ“λ” λ΅μ»¬μ—μ„ μ΄μ μ¤‘μΈ AI μ„λ²„λ¥Ό λ²¤μΉλ§ν¬ μ‹μ¤ν…μ— ν†µν•©ν•λ” λ°©λ²•μ„ μ„¤λ…ν•©λ‹λ‹¤.

## π― κ°μ”

λ΅μ»¬ AI μ„λ²„λ¥Ό λ²¤μΉλ§ν¬μ— μ¶”κ°€ν•λ” λ°©λ²•μ€ λ‘ κ°€μ§€μ…λ‹λ‹¤:

1. **λ°©λ²• A**: OpenAI νΈν™ APIλ¥Ό μ‚¬μ©ν•λ” κ²½μ° (κ¶μ¥)
2. **λ°©λ²• B**: μ»¤μ¤ν…€ APIλ¥Ό μ‚¬μ©ν•λ” κ²½μ°

---

## λ°©λ²• A: OpenAI νΈν™ API μ‚¬μ© (λΉ λ¥Έ μ„¤μ •)

λ€λ¶€λ¶„μ λ΅μ»¬ AI μ„λ²„ ν”„λ μ„μ›ν¬λ” OpenAI νΈν™ APIλ¥Ό μ κ³µν•©λ‹λ‹¤:
- vLLM
- Text Generation Inference (TGI)
- LocalAI
- LM Studio
- Ollama (μ΄λ―Έ μ§€μ›λ¨)

### 1λ‹¨κ³„: λ΅μ»¬ AI μ„λ²„ ν™•μΈ

λ¨Όμ € λ΅μ»¬ AI μ„λ²„κ°€ μ‹¤ν–‰ μ¤‘μΈμ§€ ν™•μΈν•©λ‹λ‹¤:

```bash
# μμ‹: λ΅μ»¬ μ„λ²„κ°€ http://localhost:8000μ—μ„ μ‹¤ν–‰ μ¤‘μΈ κ²½μ°
curl http://localhost:8000/v1/models

# μ‘λ‹µ μμ‹:
# {
#   "data": [
#     {"id": "my-custom-model", "object": "model", ...}
#   ]
# }
```

### 2λ‹¨κ³„: .env νμΌμ— μ„¤μ • μ¶”κ°€

`.env` νμΌμ„ μ—΄κ³  λ΅μ»¬ AI μ„λ²„ μ„¤μ •μ„ μ¶”κ°€ν•©λ‹λ‹¤:

```bash
# λ΅μ»¬ AI μ„λ²„ μ„¤μ • (OpenAI νΈν™)
LOCAL_AI_API_KEY=not_required
LOCAL_AI_MODEL=your-model-name
LOCAL_AI_BASE_URL=http://localhost:8000/v1
```

**μ¤‘μ”**:
- `LOCAL_AI_MODEL`: μ„λ²„μ—μ„ μ‚¬μ© κ°€λ¥ν• λ¨λΈ μ΄λ¦„ (μ„ curl λ…λ ΉμΌλ΅ ν™•μΈν• μ΄λ¦„)
- `LOCAL_AI_BASE_URL`: μ„λ²„ μ£Όμ† + `/v1` (OpenAI API νΈν™ μ—”λ“ν¬μΈνΈ)
- `LOCAL_AI_API_KEY`: μΈμ¦μ΄ ν•„μ” μ—†μΌλ©΄ `not_required`, ν•„μ”ν•λ©΄ μ‹¤μ  ν‚¤ μ…λ ¥

### 3λ‹¨κ³„: config.yaml νμΌ μμ •

`config/config.yaml` νμΌμ„ μ—΄κ³  `llm_providers` μ„Ήμ…μ— λ΅μ»¬ AI μ„λ²„λ¥Ό μ¶”κ°€ν•©λ‹λ‹¤:

```yaml
llm_providers:
  # ... κΈ°μ΅΄ providers ...

  local_ai:
    api_key_env: "LOCAL_AI_API_KEY"
    model_env: "LOCAL_AI_MODEL"
    base_url_env: "LOCAL_AI_BASE_URL"
```

### 4λ‹¨κ³„: λ²¤μΉλ§ν¬ μ‹¤ν–‰

μ΄μ  λ΅μ»¬ AI μ„λ²„λ΅ λ²¤μΉλ§ν¬λ¥Ό μ‹¤ν–‰ν•  μ μμµλ‹λ‹¤:

```bash
# λ°©λ²• 1: λ¨λ“  λ¨λΈ ν…μ¤νΈ (λ΅μ»¬ AI ν¬ν•¨)
python run_benchmark.py

# λ°©λ²• 2: λ΅μ»¬ AIλ§ ν…μ¤νΈ
python test_model.py --provider local_ai

# λ°©λ²• 3: νΉμ • μ—μ΄μ „νΈλ§ ν…μ¤νΈ
python test_agent.py --agent source_code --provider local_ai

# λ°©λ²• 4: λ‹¨μΌ νμΌ ν…μ¤νΈ
python test_single_file.py --file data/test_files/source_code/rsa_example.py --provider local_ai
```

---

## λ°©λ²• B: μ»¤μ¤ν…€ API μ‚¬μ© (κ³ κΈ‰ μ„¤μ •)

OpenAI νΈν™ APIκ°€ μ•„λ‹ κ²½μ°, μ»¤μ¤ν…€ ν΄λΌμ΄μ–ΈνΈλ¥Ό μƒμ„±ν•΄μ•Ό ν•©λ‹λ‹¤.

### 1λ‹¨κ³„: λ΅μ»¬ AI ν΄λΌμ΄μ–ΈνΈ ν΄λμ¤ μƒμ„±

`clients/local_ai_client.py` νμΌμ„ μƒμ„±ν•©λ‹λ‹¤:

```python
import requests
import json
from typing import Dict, Any
from .base_client import BaseLLMClient

class LocalAIClient(BaseLLMClient):
    def __init__(self, api_key: str = "not_required", model: str = "custom-model", base_url: str = "http://localhost:8000"):
        super().__init__(api_key, model, base_url)

    def make_request(self, prompt: str, max_tokens: int = 1000) -> Dict[str, Any]:
        """
        λ΅μ»¬ AI μ„λ²„μ— μ”μ²­μ„ λ³΄λ‚΄κ³  μ‘λ‹µμ„ λ°ν™ν•©λ‹λ‹¤.

        μ΄ λ©”μ„λ“λ¥Ό λ΅μ»¬ AI μ„λ²„μ API μ¤ν™μ— λ§κ² μμ •ν•μ„Έμ”.
        """
        try:
            # ===== μ—¬κΈ°λ¥Ό λ΅μ»¬ AI μ„λ²„ APIμ— λ§κ² μμ • =====
            headers = {
                "Content-Type": "application/json"
            }

            # API ν‚¤κ°€ ν•„μ”ν• κ²½μ°
            if self.api_key and self.api_key != "not_required":
                headers["Authorization"] = f"Bearer {self.api_key}"

            # μ”μ²­ λ°μ΄ν„° κµ¬μ„± (λ΅μ»¬ μ„λ²„ API μ¤ν™μ— λ§κ² μμ •)
            data = {
                "prompt": prompt,
                "max_tokens": max_tokens,
                "temperature": 0.1,
                "model": self.model
            }

            # API μ—”λ“ν¬μΈνΈ (λ΅μ»¬ μ„λ²„μ— λ§κ² μμ •)
            endpoint = f"{self.base_url}/generate"  # λλ” /chat/completions, /inference λ“±

            response = requests.post(
                endpoint,
                headers=headers,
                json=data,
                timeout=120
            )

            if response.status_code != 200:
                raise Exception(f"HTTP {response.status_code}: {response.text}")

            response_json = response.json()

            # μ‘λ‹µμ—μ„ ν…μ¤νΈ μ¶”μ¶ (λ΅μ»¬ μ„λ²„ μ‘λ‹µ ν•μ‹μ— λ§κ² μμ •)
            # μμ‹ 1: OpenAI μ¤νƒ€μΌ
            # content = response_json['choices'][0]['message']['content']

            # μμ‹ 2: μ»¤μ¤ν…€ ν•μ‹
            content = response_json.get('generated_text', '')
            # λλ”
            # content = response_json.get('response', '')
            # λλ”
            # content = response_json.get('output', '')

            # ν† ν° μ‚¬μ©λ‰ μ¶”μ • (λ΅μ»¬ μ„λ²„κ°€ μ κ³µν•μ§€ μ•λ” κ²½μ°)
            prompt_tokens = len(prompt.split()) * 1.3
            completion_tokens = len(content.split()) * 1.3
            total_tokens = int(prompt_tokens + completion_tokens)

            return {
                'content': content,
                'usage': {
                    'prompt_tokens': int(prompt_tokens),
                    'completion_tokens': int(completion_tokens),
                    'total_tokens': total_tokens
                },
                'model': self.model
            }
            # ===== μμ • λ =====

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"Local AI Error Details: {error_details}")
            raise Exception(f"Local AI API Error: {str(e)}")

    def is_available(self) -> bool:
        """λ΅μ»¬ AI μ„λ²„κ°€ μ‹¤ν–‰ μ¤‘μΈμ§€ ν™•μΈ"""
        try:
            # ν—¬μ¤ μ²΄ν¬ μ—”λ“ν¬μΈνΈ (λ΅μ»¬ μ„λ²„μ— λ§κ² μμ •)
            response = requests.get(f"{self.base_url}/health", timeout=5)
            # λλ”
            # response = requests.get(f"{self.base_url}/v1/models", timeout=5)

            return response.status_code == 200
        except:
            return False
```

### 2λ‹¨κ³„: ClientFactoryμ— λ“±λ΅

`clients/client_factory.py` νμΌμ„ μμ •ν•©λ‹λ‹¤:

```python
from typing import Dict, Any
from .base_client import BaseLLMClient
from .openai_client import OpenAIClient
from .google_client import GoogleClient
from .anthropic_client import AnthropicClient
from .xai_client import XAIClient
from .ollama_client import OllamaClient
from .local_ai_client import LocalAIClient  # μ¶”κ°€

class ClientFactory:
    _clients = {
        'openai': OpenAIClient,
        'google': GoogleClient,
        'anthropic': AnthropicClient,
        'xai': XAIClient,
        'ollama': OllamaClient,
        'local_ai': LocalAIClient  # μ¶”κ°€
    }

    # ... λ‚λ¨Έμ§€ μ½”λ“λ” λ™μΌ ...
```

### 3λ‹¨κ³„: .env λ° config.yaml μ„¤μ •

λ°©λ²• Aμ 2-3λ‹¨κ³„μ™€ λ™μΌν•κ² μ„¤μ •ν•©λ‹λ‹¤.

### 4λ‹¨κ³„: λ²¤μΉλ§ν¬ μ‹¤ν–‰

λ°©λ²• Aμ 4λ‹¨κ³„μ™€ λ™μΌν•©λ‹λ‹¤.

---

## π§ μ—°κ²° ν…μ¤νΈ

λ²¤μΉλ§ν¬λ¥Ό μ‹¤ν–‰ν•κΈ° μ „μ— μ—°κ²°μ„ ν…μ¤νΈν•  μ μμµλ‹λ‹¤:

```python
# test_local_connection.py
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from clients.local_ai_client import LocalAIClient  # λλ” OpenAIClient
from dotenv import load_dotenv

load_dotenv()

# λ΅μ»¬ AI ν΄λΌμ΄μ–ΈνΈ μƒμ„±
client = LocalAIClient(
    api_key=os.getenv("LOCAL_AI_API_KEY", "not_required"),
    model=os.getenv("LOCAL_AI_MODEL", "your-model"),
    base_url=os.getenv("LOCAL_AI_BASE_URL", "http://localhost:8000")
)

# μ—°κ²° ν…μ¤νΈ
print("Testing connection...")
if client.is_available():
    print("β… Server is available!")
else:
    print("β Server is not available")
    sys.exit(1)

# κ°„λ‹¨ν• μ”μ²­ ν…μ¤νΈ
print("\nTesting request...")
try:
    response = client.make_request(
        prompt="Analyze this code for RSA encryption: import rsa; key = rsa.generate_private_key(2048)",
        max_tokens=500
    )
    print(f"β… Request successful!")
    print(f"Response: {response['content'][:200]}...")
    print(f"Tokens: {response['usage']['total_tokens']}")
except Exception as e:
    print(f"β Request failed: {e}")
```

μ‹¤ν–‰:
```bash
python test_local_connection.py
```

---

## π“ RAG ν†µν•© μ„λ²„μ κ²½μ°

RAG μ‹μ¤ν…μ„ κ°–μ¶ λ΅μ»¬ AI μ„λ²„λ¥Ό ν…μ¤νΈν•λ” κ²½μ°:

### 1. RAG μ°Έκ³  λ¬Έμ„ ν™μ©

λ²¤μΉλ§ν¬λ¥Ό μ‹¤ν–‰ν•κΈ° μ „μ—, μƒμ„±ν• RAG μ°Έκ³  λ¬Έμ„λ¥Ό λ΅μ»¬ AI μ„λ²„μ— λ΅λ“ν•μ„Έμ”:

```bash
# RAG μ§€μ‹ λ² μ΄μ¤ νμΌλ“¤
data/rag_knowledge/source_code_agent_reference.json
data/rag_knowledge/assembly_binary_agent_reference.json
data/rag_knowledge/logs_config_agent_reference.json
```

### 2. λ²¤μΉλ§ν¬ μ‹¤ν–‰ μ‹ μ£Όμμ‚¬ν•­

RAG μ‹μ¤ν…μ€ μ¶”κ°€ μ²λ¦¬ μ‹κ°„μ΄ ν•„μ”ν•  μ μμµλ‹λ‹¤. `config/config.yaml`μ—μ„ νƒ€μ„μ•„μ›ƒμ„ μ΅°μ •ν•μ„Έμ”:

```yaml
benchmark:
  timeout_seconds: 90  # RAG μ²λ¦¬ μ‹κ°„μ„ κ³ λ ¤ν•μ—¬ μ¦κ°€
  max_retries: 3
```

### 3. μ„±λ¥ λΉ„κµ

RAGκ°€ μλ” λ¨λΈκ³Ό μ—†λ” λ¨λΈμ μ„±λ¥μ„ λΉ„κµν•λ ¤λ©΄:

```bash
# RAG μ—†λ” λ΅μ»¬ λ¨λΈ
python test_model.py --provider ollama --model llama3:8b

# RAG μλ” λ΅μ»¬ λ¨λΈ
python test_model.py --provider local_ai --model your-rag-model

# κ²°κ³Ό λΉ„κµ
python analyze_and_visualize.py benchmark_results.json
```

---

## π”§ λ¬Έμ  ν•΄κ²°

### 1. μ—°κ²° μ¤λ¥
```
Error: Connection refused
```

**ν•΄κ²° λ°©λ²•**:
- λ΅μ»¬ AI μ„λ²„κ°€ μ‹¤ν–‰ μ¤‘μΈμ§€ ν™•μΈ
- ν¬νΈ λ²νΈκ°€ μ¬λ°”λ¥Έμ§€ ν™•μΈ
- λ°©ν™”λ²½ μ„¤μ • ν™•μΈ

```bash
# μ„λ²„ μƒνƒ ν™•μΈ
curl http://localhost:8000/health

# λλ”
netstat -an | grep 8000
```

### 2. λ¨λΈμ„ μ°Ύμ„ μ μ—†μ
```
Error: Model 'your-model' not found
```

**ν•΄κ²° λ°©λ²•**:
- μ‚¬μ© κ°€λ¥ν• λ¨λΈ λ©λ΅ ν™•μΈ
```bash
curl http://localhost:8000/v1/models
```
- `.env` νμΌμ `LOCAL_AI_MODEL`μ„ μ •ν™•ν• λ¨λΈ μ΄λ¦„μΌλ΅ μμ •

### 3. νƒ€μ„μ•„μ›ƒ μ¤λ¥
```
Error: Request timeout
```

**ν•΄κ²° λ°©λ²•**:
- `config/config.yaml`μ—μ„ νƒ€μ„μ•„μ›ƒ μ¦κ°€
- λ΅μ»¬ μ„λ²„μ ν•λ“μ›¨μ–΄ λ¦¬μ†μ¤ ν™•μΈ (GPU λ©”λ¨λ¦¬, CPU μ‚¬μ©λ¥ )

### 4. JSON νμ‹± μ¤λ¥
```
Error: Invalid JSON response
```

**ν•΄κ²° λ°©λ²•**:
- λ΅μ»¬ AI μ„λ²„μ μ‘λ‹µ ν•μ‹ ν™•μΈ
- `local_ai_client.py`μ μ‘λ‹µ νμ‹± λ΅μ§ μμ •
- ν”„λ΅¬ν”„νΈμ— JSON ν•μ‹ μ”μ²­ μ¶”κ°€

---

## π“ μμƒ κ²°κ³Ό

λ΅μ»¬ AI μ„λ²„λ¥Ό λ²¤μΉλ§ν¬μ— μ¶”κ°€ν• ν›„:

1. **results/** λ””λ ‰ν† λ¦¬μ— κ²°κ³Ό JSON μƒμ„±
2. **λ¶„μ„ λ„κµ¬** μ‹¤ν–‰:
   ```bash
   python analyze_and_visualize.py benchmark_results.json
   ```

3. **μƒμ„±λλ” λ¶„μ„ κ²°κ³Ό**:
   - `COMPREHENSIVE_REPORT.txt`: μΆ…ν•© λ³΄κ³ μ„
   - `model_f1_comparison.png`: λ¨λΈλ³„ F1 Score
   - `precision_recall_f1.png`: Precision, Recall, F1 λΉ„κµ
   - `agent_performance.png`: μ—μ΄μ „νΈλ³„ μ„±λ¥
   - `model_response_time.png`: μ‘λ‹µ μ‹κ°„ λΉ„κµ

---

## π― RAG μ„±λ¥ ν‰κ°€ ν

RAG μ‹μ¤ν…μ ν¨κ³Όλ¥Ό ν‰κ°€ν•λ ¤λ©΄:

1. **Before/After λΉ„κµ**:
   - RAG μ—†μ΄: κΈ°λ³Έ μ¤ν”μ†μ¤ λ¨λΈ (Ollama llama3:8b)
   - RAG μμ: λ΅μ»¬ RAG AI μ„λ²„

2. **μ£Όμ” μ§€ν‘**:
   - **Recall (μ¬ν„μ¨)**: RAGκ°€ λ„λ½λ μ•κ³ λ¦¬μ¦μ„ νƒμ§€ν•λ”κ°€?
   - **Precision (μ •λ°€λ„)**: RAGκ°€ μ¤νƒμ„ μ¤„μ΄λ”κ°€?
   - **Korean Algorithm Detection**: ν•κµ­ μ•κ³ λ¦¬μ¦ νƒμ§€μ¨ κ°μ„ ?
   - **Response Time**: RAG μ¶”κ°€λ΅ μΈν• μ†λ„ μ €ν•?

3. **μ—μ΄μ „νΈλ³„ λ¶„μ„**:
   ```bash
   python test_agent.py --agent source_code --provider local_ai
   python test_agent.py --agent assembly_binary --provider local_ai
   python test_agent.py --agent logs_config --provider local_ai
   ```

---

## π“ μ¶”κ°€ λ¦¬μ†μ¤

- **OpenAI API νΈν™ μ„λ²„ μμ‹**:
  - [vLLM](https://github.com/vllm-project/vllm)
  - [Text Generation Inference](https://github.com/huggingface/text-generation-inference)
  - [LocalAI](https://github.com/go-skynet/LocalAI)

- **λ²¤μΉλ§ν¬ μ»¤μ¤ν„°λ§μ΄μ§•**:
  - `config/config.yaml`: λ²¤μΉλ§ν¬ μ„¤μ •
  - `agents/`: μ—μ΄μ „νΈ ν”„λ΅¬ν”„νΈ μμ •
  - `utils/metrics_calculator.py`: ν‰κ°€ μ§€ν‘ μ¶”κ°€

---

κ¶κΈν• μ μ΄ μμΌλ©΄ μ–Έμ λ“  μ§λ¬Έν•μ„Έμ”!
