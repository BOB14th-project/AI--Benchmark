#!/usr/bin/env python3
"""
ì•Œê³ ë¦¬ì¦˜ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ê° ëª¨ë¸ì´ ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ì„ ì˜ ì°¾ì•˜ëŠ”ì§€ ë¶„ì„í•˜ê³ 
í•œêµ­ ì•Œê³ ë¦¬ì¦˜ì˜ ì •í™•ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
"""

import json
from pathlib import Path
from collections import defaultdict


def load_ground_truth():
    """Ground truth ë°ì´í„° ë¡œë“œ"""
    ground_truth_dir = Path('data/ground_truth')
    ground_truth = {}

    for category_dir in ground_truth_dir.iterdir():
        if not category_dir.is_dir():
            continue

        for gt_file in category_dir.glob('*.json'):
            test_id = gt_file.stem
            try:
                with open(gt_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if 'expected_findings' in data:
                        ground_truth[test_id] = data['expected_findings']
                    elif 'vulnerable_algorithms_detected' in data:
                        # ì§ì ‘ expected_findings í˜•ì‹
                        ground_truth[test_id] = data
            except Exception as e:
                print(f"Warning: Failed to load {gt_file.name}: {e}")
                continue

    return ground_truth


def calculate_metrics(tp, fp, fn):
    """ë©”íŠ¸ë¦­ ê³„ì‚°"""
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

    return {
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'tp': tp,
        'fp': fp,
        'fn': fn
    }


def analyze_by_algorithm(results, ground_truth, with_rag=True):
    """ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥ ë¶„ì„"""
    # ê° ì•Œê³ ë¦¬ì¦˜ë³„ TP, FP, FN ê³„ì‚°
    algo_stats = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0, 'count': 0, 'test_cases': []})

    for result in results:
        if result.get('with_rag') != with_rag:
            continue
        if 'error' in result:
            continue

        test_id = result.get('test_id')
        if test_id not in ground_truth:
            continue

        gt = ground_truth[test_id]
        expected_algos = set(gt.get('vulnerable_algorithms_detected', []))
        detected_algos = set(result.get('raw_response', {}).get('detected_algorithms', []))

        # ê° ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ í‰ê°€
        for algo in expected_algos:
            algo_stats[algo]['count'] += 1
            algo_stats[algo]['test_cases'].append(test_id)

            if algo in detected_algos:
                algo_stats[algo]['tp'] += 1
            else:
                algo_stats[algo]['fn'] += 1

        # FP: íƒì§€í–ˆì§€ë§Œ ì‹¤ì œë¡œ ì—†ëŠ” ì•Œê³ ë¦¬ì¦˜
        for algo in detected_algos - expected_algos:
            algo_stats[algo]['fp'] += 1

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    algo_metrics = {}
    for algo, stats in algo_stats.items():
        metrics = calculate_metrics(stats['tp'], stats['fp'], stats['fn'])
        metrics['test_count'] = stats['count']
        algo_metrics[algo] = metrics

    return algo_metrics


def is_korean_algorithm(algo_name):
    """í•œêµ­ ì•Œê³ ë¦¬ì¦˜ì¸ì§€ í™•ì¸"""
    korean_algos = ['ARIA', 'LEA', 'HIGHT', 'SEED']
    return any(korean in algo_name.upper() for korean in korean_algos)


def analyze_model_file(file_path, ground_truth):
    """ëª¨ë¸ íŒŒì¼ ë¶„ì„"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    model_name = data['benchmark_info']['test_models'][0]
    results = data['results']

    # With RAG ë¶„ì„
    rag_algo_metrics = analyze_by_algorithm(results, ground_truth, with_rag=True)

    # Without RAG ë¶„ì„
    no_rag_algo_metrics = analyze_by_algorithm(results, ground_truth, with_rag=False)

    return {
        'model_name': model_name,
        'with_rag': rag_algo_metrics,
        'without_rag': no_rag_algo_metrics
    }


def print_algorithm_comparison(all_models_analysis):
    """ì•Œê³ ë¦¬ì¦˜ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì¶œë ¥"""

    print("\n" + "="*100)
    print("ğŸ” ì•Œê³ ë¦¬ì¦˜ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (With RAG)")
    print("="*100)

    # ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ìˆ˜ì§‘
    all_algorithms = set()
    for analysis in all_models_analysis:
        all_algorithms.update(analysis['with_rag'].keys())

    all_algorithms = sorted(list(all_algorithms))

    # ì•Œê³ ë¦¬ì¦˜ë³„ë¡œ ë¹„êµ
    for algo in all_algorithms:
        korean_marker = " ğŸ‡°ğŸ‡·" if is_korean_algorithm(algo) else ""
        print(f"\nğŸ“Š {algo}{korean_marker}")
        print("-" * 100)

        model_scores = []
        for analysis in all_models_analysis:
            model_name = analysis['model_name']
            if algo in analysis['with_rag']:
                metrics = analysis['with_rag'][algo]
                model_scores.append((model_name, metrics['f1_score'], metrics))
            else:
                model_scores.append((model_name, 0.0, None))

        # F1 ìŠ¤ì½”ì–´ë¡œ ì •ë ¬
        model_scores.sort(key=lambda x: x[1], reverse=True)

        for rank, (model_name, f1_score, metrics) in enumerate(model_scores, 1):
            if metrics:
                print(f"  {rank}. {model_name:20s} | F1: {f1_score:.4f} | "
                      f"Precision: {metrics['precision']:.4f} | "
                      f"Recall: {metrics['recall']:.4f} | "
                      f"TP: {metrics['tp']:3d} | FP: {metrics['fp']:3d} | FN: {metrics['fn']:3d}")
            else:
                print(f"  {rank}. {model_name:20s} | No data")


def print_korean_algorithm_summary(all_models_analysis):
    """í•œêµ­ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ìš”ì•½"""

    print("\n" + "="*100)
    print("ğŸ‡°ğŸ‡· í•œêµ­ ì•Œê³ ë¦¬ì¦˜ íƒì§€ ì •í™•ë„ ë¹„êµ")
    print("="*100)

    korean_algos_found = set()
    for analysis in all_models_analysis:
        for algo in analysis['with_rag'].keys():
            if is_korean_algorithm(algo):
                korean_algos_found.add(algo)

    if not korean_algos_found:
        print("\nâš ï¸  í•œêµ­ ì•Œê³ ë¦¬ì¦˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    korean_algos_found = sorted(list(korean_algos_found))

    print("\nğŸ“Œ With RAG ì„±ëŠ¥:")
    print("-" * 100)

    for algo in korean_algos_found:
        print(f"\n{algo}:")

        model_results = []
        for analysis in all_models_analysis:
            model_name = analysis['model_name']
            if algo in analysis['with_rag']:
                metrics = analysis['with_rag'][algo]
                model_results.append((model_name, metrics))

        # F1 ìŠ¤ì½”ì–´ë¡œ ì •ë ¬
        model_results.sort(key=lambda x: x[1]['f1_score'], reverse=True)

        for rank, (model_name, metrics) in enumerate(model_results, 1):
            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰"
            print(f"  {medal} {model_name:20s} | "
                  f"F1: {metrics['f1_score']:.4f} | "
                  f"Precision: {metrics['precision']:.4f} | "
                  f"Recall: {metrics['recall']:.4f}")

    # RAG vs No RAG ë¹„êµ
    print("\n" + "-" * 100)
    print("ğŸ“Œ RAG íš¨ê³¼ ë¹„êµ (í•œêµ­ ì•Œê³ ë¦¬ì¦˜):")
    print("-" * 100)

    for model_analysis in all_models_analysis:
        model_name = model_analysis['model_name']
        print(f"\n{model_name}:")

        for algo in korean_algos_found:
            rag_f1 = 0.0
            no_rag_f1 = 0.0

            if algo in model_analysis['with_rag']:
                rag_f1 = model_analysis['with_rag'][algo]['f1_score']
            if algo in model_analysis['without_rag']:
                no_rag_f1 = model_analysis['without_rag'][algo]['f1_score']

            improvement = ((rag_f1 - no_rag_f1) / no_rag_f1 * 100) if no_rag_f1 > 0 else 0
            arrow = "ğŸ“ˆ" if improvement > 0 else "ğŸ“‰" if improvement < 0 else "â¡ï¸"

            print(f"  {algo:15s} | With RAG: {rag_f1:.4f} | Without RAG: {no_rag_f1:.4f} | "
                  f"{arrow} {improvement:+6.1f}%")


def print_model_summary(all_models_analysis):
    """ëª¨ë¸ë³„ ì „ì²´ ì„±ëŠ¥ ìš”ì•½"""

    print("\n" + "="*100)
    print("ğŸ“ˆ ëª¨ë¸ë³„ ì „ì²´ ì„±ëŠ¥ ìš”ì•½ (With RAG)")
    print("="*100)

    model_summaries = []

    for analysis in all_models_analysis:
        model_name = analysis['model_name']
        algo_metrics = analysis['with_rag']

        # ì „ì²´ TP, FP, FN í•©ì‚°
        total_tp = sum(m['tp'] for m in algo_metrics.values())
        total_fp = sum(m['fp'] for m in algo_metrics.values())
        total_fn = sum(m['fn'] for m in algo_metrics.values())

        overall_metrics = calculate_metrics(total_tp, total_fp, total_fn)

        # í•œêµ­ ì•Œê³ ë¦¬ì¦˜ë§Œ ë”°ë¡œ ê³„ì‚°
        korean_tp = sum(m['tp'] for algo, m in algo_metrics.items() if is_korean_algorithm(algo))
        korean_fp = sum(m['fp'] for algo, m in algo_metrics.items() if is_korean_algorithm(algo))
        korean_fn = sum(m['fn'] for algo, m in algo_metrics.items() if is_korean_algorithm(algo))
        korean_metrics = calculate_metrics(korean_tp, korean_fp, korean_fn)

        model_summaries.append({
            'name': model_name,
            'overall': overall_metrics,
            'korean': korean_metrics
        })

    # F1 ìŠ¤ì½”ì–´ë¡œ ì •ë ¬
    model_summaries.sort(key=lambda x: x['overall']['f1_score'], reverse=True)

    print("\nì „ì²´ ì•Œê³ ë¦¬ì¦˜:")
    for rank, summary in enumerate(model_summaries, 1):
        medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰"
        metrics = summary['overall']
        print(f"{medal} {rank}. {summary['name']:20s} | "
              f"F1: {metrics['f1_score']:.4f} | "
              f"Precision: {metrics['precision']:.4f} | "
              f"Recall: {metrics['recall']:.4f}")

    # í•œêµ­ ì•Œê³ ë¦¬ì¦˜ ìˆœìœ„
    model_summaries.sort(key=lambda x: x['korean']['f1_score'], reverse=True)

    print("\ní•œêµ­ ì•Œê³ ë¦¬ì¦˜ë§Œ:")
    for rank, summary in enumerate(model_summaries, 1):
        medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰"
        metrics = summary['korean']
        print(f"{medal} {rank}. {summary['name']:20s} | "
              f"F1: {metrics['f1_score']:.4f} | "
              f"Precision: {metrics['precision']:.4f} | "
              f"Recall: {metrics['recall']:.4f}")


def main():
    results_dir = Path('results')

    files = {
        'llama': results_dir / 'llama_final.json',
        'gemini': results_dir / 'gemini_final.json',
        'gpt': results_dir / 'gpt_final.json'
    }

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    for name, file_path in files.items():
        if not file_path.exists():
            print(f"Error: File not found: {file_path}")
            return

    print("\n" + "="*100)
    print("ğŸ”¬ ì•Œê³ ë¦¬ì¦˜ë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„")
    print("="*100)

    # Ground truth ë¡œë“œ
    print("Loading ground truth data...")
    ground_truth = load_ground_truth()
    print(f"Loaded {len(ground_truth)} test cases")

    # ëª¨ë“  ëª¨ë¸ ë¶„ì„
    all_models_analysis = []
    for name, file_path in files.items():
        print(f"Loading: {file_path.name}")
        analysis = analyze_model_file(file_path, ground_truth)
        all_models_analysis.append(analysis)

    # 1. ëª¨ë¸ë³„ ì „ì²´ ì„±ëŠ¥ ìš”ì•½
    print_model_summary(all_models_analysis)

    # 2. ì•Œê³ ë¦¬ì¦˜ë³„ ë¹„êµ
    print_algorithm_comparison(all_models_analysis)

    # 3. í•œêµ­ ì•Œê³ ë¦¬ì¦˜ ìš”ì•½
    print_korean_algorithm_summary(all_models_analysis)

    print("\n" + "="*100)
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print("="*100 + "\n")


if __name__ == "__main__":
    main()
