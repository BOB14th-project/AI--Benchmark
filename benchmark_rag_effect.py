#!/usr/bin/env python3
"""
RAG íš¨ê³¼ ì¸¡ì • ë²¤ì¹˜ë§ˆí¬

ê°™ì€ ê¸°ë³¸ ëª¨ë¸ë¡œ RAG ìœ ë¬´ì— ë”°ë¥¸ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤:
- PQC Inspector (RAG í¬í•¨) vs ê¸°ë³¸ ëª¨ë¸ (RAG ì—†ìŒ)
"""

import os
import json
import time
import argparse
import subprocess
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

from clients.pqc_inspector_client import PQCInspectorClient
from clients.ollama_client import OllamaClient
from clients.google_client import GoogleClient
from agents.agent_factory import AgentFactory
from utils.test_case_manager import TestCaseManager
from config.config_loader import ConfigLoader


class RAGEffectBenchmark:
    """
    RAG íš¨ê³¼ ì¸¡ì • ë²¤ì¹˜ë§ˆí¬

    ë¹„êµ ëª¨ë¸:
    1. llama3:8b (PQC Inspector + RAG) vs llama3:8b (ìˆœìˆ˜)
    2. gemini-2.0-flash-exp (PQC Inspector + RAG) vs gemini-2.0-flash-exp (ìˆœìˆ˜)
    """

    def __init__(self, pqc_base_url: str = "http://localhost:8000",
                 ollama_base_url: str = "http://localhost:11434"):
        self.pqc_base_url = pqc_base_url
        self.ollama_base_url = ollama_base_url

        # ConfigLoader ì´ˆê¸°í™”
        self.config_loader = ConfigLoader()

        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë§¤ë‹ˆì €
        self.test_manager = TestCaseManager(
            test_cases_dir="data/test_cases",
            ground_truth_dir="data/ground_truth",
            test_files_dir="data/test_files"
        )

        # ê²°ê³¼ ì €ì¥
        self.results = []

        # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ëª©ë¡
        self.test_models = []

    def setup_test_models(self, models: List[str] = None):
        """í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ì„¤ì •"""
        if models:
            self.test_models = models
        else:
            # ê¸°ë³¸ ëª¨ë¸ë“¤
            self.test_models = [
                "llama3:8b",
                "gemini-2.0-flash-exp"
            ]

        print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ëª¨ë¸: {self.test_models}")

    def restart_ai_server_with_model(self, model: str, agent_type: str) -> bool:
        """
        AI-Serverì˜ ì—ì´ì „íŠ¸ ëª¨ë¸ì„ ë³€ê²½í•˜ê³  ì¬ì‹œì‘

        ì£¼ì˜: ì‹¤ì œë¡œëŠ” .env íŒŒì¼ì„ ìˆ˜ì •í•˜ê³  ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.
        ì´ ë©”ì„œë“œëŠ” ìë™í™”ëœ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.
        """
        print(f"\nğŸ”„ AI-Server ì„¤ì • ë³€ê²½: {agent_type} â†’ {model}")

        # .env íŒŒì¼ ê²½ë¡œ
        env_file = Path("../AI-Server/.env")

        if not env_file.exists():
            print(f"âŒ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {env_file}")
            return False

        # ì—ì´ì „íŠ¸ íƒ€ì…ì— ë”°ë¥¸ í™˜ê²½ ë³€ìˆ˜ ì´ë¦„
        env_var_map = {
            'source_code': 'SOURCE_CODE_MODEL',
            'assembly_binary': 'BINARY_MODEL',
            'logs_config': 'LOG_CONF_MODEL'
        }

        if agent_type not in env_var_map:
            print(f"âŒ ì˜ëª»ëœ ì—ì´ì „íŠ¸ íƒ€ì…: {agent_type}")
            return False

        env_var_name = env_var_map[agent_type]

        # .env íŒŒì¼ ì½ê¸°
        with open(env_file, 'r') as f:
            lines = f.readlines()

        # í•´ë‹¹ ë³€ìˆ˜ ì°¾ì•„ì„œ ìˆ˜ì •
        modified = False
        new_lines = []
        for line in lines:
            if line.startswith(f"{env_var_name}="):
                new_lines.append(f"{env_var_name}={model}\n")
                modified = True
            else:
                new_lines.append(line)

        if not modified:
            # ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
            new_lines.append(f"\n{env_var_name}={model}\n")

        # .env íŒŒì¼ ì“°ê¸°
        with open(env_file, 'w') as f:
            f.writelines(new_lines)

        print(f"âœ… .env íŒŒì¼ ì—…ë°ì´íŠ¸: {env_var_name}={model}")
        print(f"âš ï¸  ìˆ˜ë™ìœ¼ë¡œ AI-Serverë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”!")
        print(f"   cd AI-Server && python main.py")

        # ì‚¬ìš©ìì—ê²Œ ì¬ì‹œì‘ í™•ì¸ ìš”ì²­
        input(f"\nğŸ”„ AI-Serverë¥¼ ì¬ì‹œì‘í•œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")

        return True

    def check_servers(self) -> bool:
        """ì„œë²„ ìƒíƒœ í™•ì¸"""
        print("=" * 80)
        print("ğŸ” ì„œë²„ ìƒíƒœ í™•ì¸")
        print("=" * 80)

        # PQC Inspector í™•ì¸
        try:
            pqc_client = PQCInspectorClient(base_url=self.pqc_base_url)
            if pqc_client.is_available():
                print(f"âœ… PQC Inspector ì„œë²„ ì‹¤í–‰ ì¤‘: {self.pqc_base_url}")
            else:
                print(f"âŒ PQC Inspector ì„œë²„ ì ‘ì† ë¶ˆê°€: {self.pqc_base_url}")
                return False
        except Exception as e:
            print(f"âŒ PQC Inspector í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

        # Ollama í™•ì¸ (llama3:8b ëª¨ë¸ ì‚¬ìš©ì‹œ)
        if any("llama" in m or ":" in m for m in self.test_models):
            try:
                ollama_client = OllamaClient(base_url=self.ollama_base_url)
                available_models = ollama_client.list_available_models()
                print(f"âœ… Ollama ì„œë²„ ì‹¤í–‰ ì¤‘: {self.ollama_base_url}")
                print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models}")
            except Exception as e:
                print(f"âŒ Ollama í™•ì¸ ì‹¤íŒ¨: {e}")
                return False

        print("=" * 80)
        return True

    def run_benchmark(self, limit_per_agent: int = None, agent_filter: List[str] = None,
                      auto_restart: bool = False):
        """
        RAG íš¨ê³¼ ì¸¡ì • ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

        Args:
            limit_per_agent: ì—ì´ì „íŠ¸ë‹¹ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìˆ˜ ì œí•œ
            agent_filter: í…ŒìŠ¤íŠ¸í•  ì—ì´ì „íŠ¸ ì„ íƒ
            auto_restart: AI-Server ìë™ ì¬ì‹œì‘ ì—¬ë¶€
        """
        if not self.check_servers():
            print("\nâŒ ì„œë²„ í™•ì¸ ì‹¤íŒ¨. ë²¤ì¹˜ë§ˆí¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        print("\n" + "=" * 80)
        print("ğŸš€ RAG íš¨ê³¼ ì¸¡ì • ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print("=" * 80)

        # ì—ì´ì „íŠ¸ íƒ€ì…
        agent_types = ['source_code', 'assembly_binary', 'logs_config']
        if agent_filter:
            agent_types = [a for a in agent_types if a in agent_filter]

        # ê° ëª¨ë¸ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
        for model in self.test_models:
            print(f"\n{'='*80}")
            print(f"ğŸ“Š ëª¨ë¸: {model}")
            print(f"{'='*80}")

            for agent_type in agent_types:
                # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ
                test_cases = self.test_manager.load_test_cases(agent_type)
                if limit_per_agent:
                    test_cases = test_cases[:limit_per_agent]

                print(f"\nğŸ“ {agent_type}: {len(test_cases)}ê°œ í…ŒìŠ¤íŠ¸")

                # AI-Server ëª¨ë¸ ë³€ê²½ (ìë™ ì¬ì‹œì‘ ëª¨ë“œ)
                if auto_restart:
                    self.restart_ai_server_with_model(model, agent_type)

                for idx, test_case in enumerate(test_cases, 1):
                    file_name = test_case.get('file_name') or Path(test_case.get('file_path', 'unknown')).name
                    print(f"\n--- í…ŒìŠ¤íŠ¸ {idx}/{len(test_cases)}: {file_name} ---")

                    # 1. PQC Inspector (RAG í¬í•¨) í…ŒìŠ¤íŠ¸
                    rag_result = self.test_with_rag(model, agent_type, test_case)
                    if rag_result:
                        self.results.append(rag_result)
                        print(f"âœ… {model} + RAG: TP={rag_result.get('true_positives', 0)}, "
                              f"FP={rag_result.get('false_positives', 0)}, "
                              f"FN={rag_result.get('false_negatives', 0)}, "
                              f"ì‹œê°„={rag_result.get('response_time', 0):.2f}ì´ˆ")

                    # 2. ê¸°ë³¸ ëª¨ë¸ (RAG ì—†ìŒ) í…ŒìŠ¤íŠ¸
                    base_result = self.test_without_rag(model, agent_type, test_case)
                    if base_result:
                        self.results.append(base_result)
                        print(f"âœ… {model} (ìˆœìˆ˜): TP={base_result.get('true_positives', 0)}, "
                              f"FP={base_result.get('false_positives', 0)}, "
                              f"FN={base_result.get('false_negatives', 0)}, "
                              f"ì‹œê°„={base_result.get('response_time', 0):.2f}ì´ˆ")

        print("\n" + "=" * 80)
        print(f"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: ì´ {len(self.results)} í…ŒìŠ¤íŠ¸")
        print("=" * 80)

    def test_with_rag(self, model: str, agent_type: str, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """PQC Inspector (RAG í¬í•¨) í…ŒìŠ¤íŠ¸"""
        try:
            # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            client = PQCInspectorClient(
                model=agent_type,
                base_url=self.pqc_base_url
            )

            # íŒŒì¼ ê²½ë¡œ
            file_path = test_case.get('file_path', '')
            if not Path(file_path).exists():
                print(f"âš ï¸  íŒŒì¼ ì—†ìŒ: {file_path}")
                # íŒŒì¼ ì—†ìŒë„ ì—ëŸ¬ë¡œ ì²˜ë¦¬
                return {
                    'base_model': model,
                    'with_rag': True,
                    'agent_type': agent_type,
                    'test_id': test_case.get('test_id'),
                    'file_name': test_case.get('file_name') or Path(test_case.get('file_path', 'unknown')).name,
                    'response_time': 0,
                    'json_valid': False,
                    'true_positives': 0,
                    'false_positives': 0,
                    'false_negatives': 0,
                    'error': 'file not found',
                    'raw_response': {}
                }

            # ë¶„ì„ ì‹¤í–‰
            prompt = f"FILE_PATH:{file_path}"
            result = client.benchmark_request(prompt)

            if not result.get('success'):
                error_msg = result.get('error', 'unknown error')
                print(f"âŒ RAG í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {error_msg}")

                # ì—ëŸ¬ ì¼€ì´ìŠ¤ëŠ” ê²°ê³¼ì— í¬í•¨í•˜ë˜, F1 ê³„ì‚°ì—ì„œ ì œì™¸í•˜ê¸° ìœ„í•´ í”Œë˜ê·¸ ì¶”ê°€
                return {
                    'base_model': model,
                    'with_rag': True,
                    'agent_type': agent_type,
                    'test_id': test_case.get('test_id'),
                    'file_name': test_case.get('file_name') or Path(test_case.get('file_path', 'unknown')).name,
                    'response_time': 0,
                    'json_valid': False,
                    'true_positives': 0,
                    'false_positives': 0,
                    'false_negatives': 0,
                    'error': error_msg,  # ì—ëŸ¬ í”Œë˜ê·¸
                    'raw_response': {}
                }

            # ì‘ë‹µ íŒŒì‹±
            response_content = result.get('content', '{}')
            try:
                analysis_result = json.loads(response_content)
            except json.JSONDecodeError:
                analysis_result = {}

            # Ground truth ë¡œë“œ
            test_id = test_case.get('test_id')
            ground_truth = self.test_manager.load_ground_truth(agent_type, test_id)

            # RAG ì‘ë‹µì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if 'detected_algorithms' in analysis_result:
                # PQC Inspector ê°„ë‹¨í•œ í˜•ì‹ â†’ analysis_results í˜•ì‹ ë³€í™˜
                detected_algs = analysis_result.get('detected_algorithms', [])
                algorithms_text = ', '.join(detected_algs)

                # ê° ì•Œê³ ë¦¬ì¦˜ì„ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ì— ë§¤í•‘
                analysis_points = {}
                for alg in detected_algs:
                    alg_lower = alg.lower()
                    if 'rsa' in alg_lower:
                        analysis_points['quantum_vulnerable_rsa_implementations_and_usage_patterns'] = f"DETECTED: {alg}"
                    if 'ecc' in alg_lower or 'ecdsa' in alg_lower or 'ecdh' in alg_lower:
                        analysis_points['elliptic_curve_cryptography_ecc_ecdsa_ecdh_implementations'] = f"DETECTED: {alg}"
                    if 'dsa' in alg_lower or 'dh' in alg_lower:
                        analysis_points['discrete_logarithm_based_algorithms_dsa_dh_elgamal'] = f"DETECTED: {alg}"
                    if any(k in alg_lower for k in ['seed', 'aria', 'hight', 'lea', 'kcdsa']):
                        analysis_points['korean_domestic_algorithms_seed_aria_hight_lea_kcdsa_ec_kcdsa_has_160_lsh'] = f"DETECTED: {alg}"

                # ë‚˜ë¨¸ì§€ëŠ” NOT DETECTEDë¡œ ì±„ì›€
                all_points = [
                    'quantum_vulnerable_rsa_implementations_and_usage_patterns',
                    'elliptic_curve_cryptography_ecc_ecdsa_ecdh_implementations',
                    'discrete_logarithm_based_algorithms_dsa_dh_elgamal',
                    'korean_domestic_algorithms_seed_aria_hight_lea_kcdsa_ec_kcdsa_has_160_lsh',
                    'symmetric_ciphers_vulnerable_to_grover\'s_algorithm_aes_128_3des_des_rc4',
                    'weak_hash_functions_md5_sha_1_sha_256_with_reduced_security'
                ]

                for point in all_points:
                    if point not in analysis_points:
                        analysis_points[point] = "NOT DETECTED"

                standardized_response = {
                    'valid_json': True,
                    'analysis_results': analysis_points,
                    'confidence_score': analysis_result.get('confidence_score', 0.0)
                }
            else:
                # ì´ë¯¸ analysis_results í˜•ì‹
                standardized_response = {
                    'valid_json': True,
                    'analysis_results': analysis_result
                }

            # ë©”íŠ¸ë¦­ ê³„ì‚° - Ground truthì™€ ì‹¤ì œ ë¹„êµ
            if ground_truth and 'expected_findings' in ground_truth:
                expected_algs = set()
                vulnerable_algs = ground_truth['expected_findings'].get('vulnerable_algorithms_detected', [])
                expected_algs = set([alg.lower() for alg in vulnerable_algs])

                # íƒì§€ëœ ì•Œê³ ë¦¬ì¦˜ ì¶”ì¶œ ë° ì •ê·œí™”
                detected_algs_raw = set()
                if 'detected_algorithms' in analysis_result:
                    detected_algs_raw = set([alg.lower() for alg in analysis_result['detected_algorithms']])

                # ì˜ˆìƒ ì•Œê³ ë¦¬ì¦˜ê³¼ ë§¤ì¹­ (ë³€í˜• ê³ ë ¤)
                matched_expected = set()  # ë§¤ì¹­ëœ expected ì•Œê³ ë¦¬ì¦˜
                for detected in detected_algs_raw:
                    for expected in expected_algs:
                        # ì•Œê³ ë¦¬ì¦˜ ë³€í˜• ë§¤ì¹­
                        if expected == detected:
                            matched_expected.add(expected)
                            break
                        elif expected == 'ecc' and detected in ['ecdsa', 'ecdh', 'ecc']:
                            matched_expected.add(expected)
                            break
                        elif expected == 'rsa' and 'rsa' in detected:
                            matched_expected.add(expected)
                            break
                        elif expected == 'dsa' and detected in ['dsa', 'ecdsa']:
                            matched_expected.add(expected)
                            break
                        elif expected == 'aes' and 'aes' in detected:
                            matched_expected.add(expected)
                            break
                        elif expected == 'md5' and 'md5' in detected:
                            matched_expected.add(expected)
                            break

                # TP, FP, FN ê³„ì‚°
                true_positives = len(matched_expected)
                false_positives = len(detected_algs_raw) - true_positives
                false_negatives = len(expected_algs) - true_positives

                # ê°œë³„ ë©”íŠ¸ë¦­ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ (ì „ì²´ í•©ì‚° í›„ ê³„ì‚°)
                metrics = {
                    'true_positives': true_positives,
                    'false_positives': false_positives,
                    'false_negatives': false_negatives
                }
            else:
                metrics = {
                    'true_positives': 0,
                    'false_positives': 0,
                    'false_negatives': 0
                }

            return {
                'base_model': model,
                'with_rag': True,
                'agent_type': agent_type,
                'test_id': test_case.get('test_id'),
                'file_name': test_case.get('file_name') or Path(test_case.get('file_path', 'unknown')).name,
                'response_time': result.get('response_time', 0),
                'json_valid': result.get('json_valid', False),
                **metrics,
                'raw_response': analysis_result
            }

        except Exception as e:
            print(f"âŒ RAG í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None

    def test_without_rag(self, model: str, agent_type: str, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ ëª¨ë¸ (RAG ì—†ìŒ) í…ŒìŠ¤íŠ¸"""
        try:
            # ëª¨ë¸ì— ë”°ë¼ í´ë¼ì´ì–¸íŠ¸ ì„ íƒ
            if ":" in model:  # Ollama ëª¨ë¸
                client = OllamaClient(
                    model=model,
                    base_url=self.ollama_base_url
                )
            elif model.startswith("gemini-"):
                config = self.config_loader.get_llm_config('google')
                client = GoogleClient(
                    api_key=config['api_key'],
                    model=model,
                    base_url=config.get('base_url', '')
                )
            else:
                print(f"âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model}")
                return None

            # ì—ì´ì „íŠ¸ ìƒì„± (í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìš©)
            agent = AgentFactory.create_agent(agent_type)

            # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            input_data = test_case.get('input_data', '')
            if len(input_data) > 4000:
                input_data = input_data[:4000] + "\n... (truncated)"

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = agent.create_prompt(input_data)

            # API í˜¸ì¶œ
            result = client.benchmark_request(prompt)

            if not result.get('success'):
                error_msg = result.get('error', 'unknown error')
                print(f"âŒ ê¸°ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (safety issue): {error_msg}")

                # ì—ëŸ¬ ì¼€ì´ìŠ¤ëŠ” ê²°ê³¼ì— í¬í•¨í•˜ë˜, F1 ê³„ì‚°ì—ì„œ ì œì™¸í•˜ê¸° ìœ„í•´ í”Œë˜ê·¸ ì¶”ê°€
                return {
                    'base_model': model,
                    'with_rag': False,
                    'agent_type': agent_type,
                    'test_id': test_case.get('test_id'),
                    'file_name': test_case.get('file_name') or Path(test_case.get('file_path', 'unknown')).name,
                    'response_time': 0,
                    'json_valid': False,
                    'true_positives': 0,
                    'false_positives': 0,
                    'false_negatives': 0,
                    'error': error_msg,  # ì—ëŸ¬ í”Œë˜ê·¸
                    'raw_response': {}
                }

            # ì‘ë‹µ íŒŒì‹±
            response_content = result.get('content', '')
            try:
                if '{' in response_content and '}' in response_content:
                    json_start = response_content.find('{')
                    json_end = response_content.rfind('}') + 1
                    json_text = response_content[json_start:json_end]
                    analysis_result = json.loads(json_text)
                else:
                    analysis_result = {}
            except json.JSONDecodeError:
                analysis_result = {}

            # Ground truth ë¡œë“œ
            test_id = test_case.get('test_id')
            ground_truth = self.test_manager.load_ground_truth(agent_type, test_id)

            # ì‘ë‹µì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            standardized_response = {
                'valid_json': True,
                'analysis_results': analysis_result
            }

            # ë©”íŠ¸ë¦­ ê³„ì‚° - Ground truthì™€ ì‹¤ì œ ë¹„êµ
            if ground_truth and 'expected_findings' in ground_truth:
                expected_algs = set()
                vulnerable_algs = ground_truth['expected_findings'].get('vulnerable_algorithms_detected', [])
                expected_algs = set([alg.lower() for alg in vulnerable_algs])

                # íƒì§€ëœ ì•Œê³ ë¦¬ì¦˜ ì¶”ì¶œ (analysis_resultsì—ì„œ)
                matched_expected = set()
                total_detected_count = 0
                if 'analysis_results' in analysis_result:
                    # analysis_resultsì—ì„œ "DETECTED:" í¬í•¨ëœ í•­ëª©ë§Œ í™•ì¸
                    analysis_results = analysis_result['analysis_results']
                    if isinstance(analysis_results, dict):
                        for key, value in analysis_results.items():
                            value_str = str(value).lower()
                            # "NOT DETECTED" ì œì™¸, "DETECTED:" í¬í•¨ë§Œ
                            if 'detected:' in value_str and 'not detected' not in value_str:
                                total_detected_count += 1
                                # ì˜ˆìƒ ì•Œê³ ë¦¬ì¦˜ê³¼ ë§¤ì¹­
                                for alg in expected_algs:
                                    # ì•Œê³ ë¦¬ì¦˜ ë³€í˜•ë„ ê³ ë ¤ (ECC = ECDSA, ECDH ë“±)
                                    alg_variants = [alg]
                                    if alg == 'ecc':
                                        alg_variants.extend(['ecdsa', 'ecdh', 'elliptic'])
                                    elif alg == 'rsa':
                                        alg_variants.extend(['rsa'])
                                    elif alg == 'dsa':
                                        alg_variants.extend(['dsa'])
                                    elif alg == 'aes':
                                        alg_variants.extend(['aes'])
                                    elif alg == 'md5':
                                        alg_variants.extend(['md5'])

                                    if any(variant in value_str or variant in key.lower() for variant in alg_variants):
                                        matched_expected.add(alg)
                                        break

                # TP, FP, FN ê³„ì‚°
                true_positives = len(matched_expected)
                false_positives = total_detected_count - true_positives
                false_negatives = len(expected_algs) - true_positives

                # ê°œë³„ ë©”íŠ¸ë¦­ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ (ì „ì²´ í•©ì‚° í›„ ê³„ì‚°)
                metrics = {
                    'true_positives': true_positives,
                    'false_positives': false_positives,
                    'false_negatives': false_negatives
                }
            else:
                metrics = {
                    'true_positives': 0,
                    'false_positives': 0,
                    'false_negatives': 0
                }

            return {
                'base_model': model,
                'with_rag': False,
                'agent_type': agent_type,
                'test_id': test_case.get('test_id'),
                'file_name': test_case.get('file_name') or Path(test_case.get('file_path', 'unknown')).name,
                'response_time': result.get('response_time', 0),
                'json_valid': result.get('json_valid', False),
                **metrics,
                'raw_response': analysis_result
            }

        except Exception as e:
            print(f"âŒ ê¸°ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None

    def save_results(self, output_file: str = None):
        """ê²°ê³¼ ì €ì¥"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"rag_effect_comparison_{timestamp}.json"

        output_path = Path("results") / output_file
        output_path.parent.mkdir(exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                'benchmark_info': {
                    'timestamp': datetime.now().isoformat(),
                    'test_models': self.test_models,
                    'pqc_base_url': self.pqc_base_url,
                    'ollama_base_url': self.ollama_base_url,
                    'total_tests': len(self.results)
                },
                'results': self.results
            }, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {output_path}")
        return output_path

    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥ - RAG íš¨ê³¼ ì¤‘ì‹¬ (ì „ì²´ TP/FP/FN í•©ì‚° ë°©ì‹)"""
        if not self.results:
            print("\nâš ï¸  ê²°ê³¼ ì—†ìŒ")
            return

        print("\n" + "=" * 80)
        print("ğŸ“Š RAG íš¨ê³¼ ì¸¡ì • ê²°ê³¼ ìš”ì•½")
        print("=" * 80)

        def calc_avg(results, key):
            vals = [r.get(key, 0) for r in results if r.get(key) is not None]
            return sum(vals) / len(vals) if vals else 0

        def calc_metrics_from_sum(results):
            """ì „ì²´ TP, FP, FN í•©ì‚° í›„ ë©”íŠ¸ë¦­ ê³„ì‚° (ì—ëŸ¬ ì¼€ì´ìŠ¤ ì œì™¸)"""
            # 'error' í”Œë˜ê·¸ê°€ ì—†ëŠ” ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
            valid_results = [r for r in results if 'error' not in r]

            total_tp = sum(r.get('true_positives', 0) for r in valid_results)
            total_fp = sum(r.get('false_positives', 0) for r in valid_results)
            total_fn = sum(r.get('false_negatives', 0) for r in valid_results)

            precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
            recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

            return {
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score,
                'tp': total_tp,
                'fp': total_fp,
                'fn': total_fn,
                'valid_count': len(valid_results),
                'error_count': len(results) - len(valid_results)
            }

        # ëª¨ë¸ë³„ë¡œ ì§‘ê³„
        for model in self.test_models:
            print(f"\nğŸ”¬ ëª¨ë¸: {model}")
            print("=" * 80)

            rag_results = [r for r in self.results if r['base_model'] == model and r['with_rag']]
            no_rag_results = [r for r in self.results if r['base_model'] == model and not r['with_rag']]

            # ì „ì²´ TP/FP/FN í•©ì‚° í›„ ë©”íŠ¸ë¦­ ê³„ì‚°
            rag_metrics = calc_metrics_from_sum(rag_results)
            no_rag_metrics = calc_metrics_from_sum(no_rag_results)

            improvement = ((rag_metrics['f1_score'] - no_rag_metrics['f1_score']) / no_rag_metrics['f1_score'] * 100) if no_rag_metrics['f1_score'] > 0 else 0

            print(f"\nğŸ“ˆ ì „ì²´ ê²°ê³¼ (TP/FP/FN í•©ì‚° ë°©ì‹, ì—ëŸ¬ ì¼€ì´ìŠ¤ ì œì™¸):")
            print(f"   RAG í¬í•¨:  F1={rag_metrics['f1_score']:.3f}, Precision={rag_metrics['precision']:.3f}, "
                  f"Recall={rag_metrics['recall']:.3f}, TP={rag_metrics['tp']}, FP={rag_metrics['fp']}, FN={rag_metrics['fn']}, "
                  f"ì‹œê°„={calc_avg(rag_results, 'response_time'):.2f}ì´ˆ "
                  f"(ì„±ê³µ: {rag_metrics['valid_count']}, ì—ëŸ¬: {rag_metrics['error_count']})")
            print(f"   RAG ì—†ìŒ:  F1={no_rag_metrics['f1_score']:.3f}, Precision={no_rag_metrics['precision']:.3f}, "
                  f"Recall={no_rag_metrics['recall']:.3f}, TP={no_rag_metrics['tp']}, FP={no_rag_metrics['fp']}, FN={no_rag_metrics['fn']}, "
                  f"ì‹œê°„={calc_avg(no_rag_results, 'response_time'):.2f}ì´ˆ "
                  f"(ì„±ê³µ: {no_rag_metrics['valid_count']}, ì—ëŸ¬: {no_rag_metrics['error_count']})")
            print(f"   ğŸ¯ RAG íš¨ê³¼: F1 Score {improvement:+.1f}% í–¥ìƒ")

            # ì—ì´ì „íŠ¸ë³„ ì§‘ê³„
            agent_types = set(r['agent_type'] for r in self.results if r['base_model'] == model)
            print(f"\nğŸ“‹ ì—ì´ì „íŠ¸ë³„ RAG íš¨ê³¼:")
            for agent_type in sorted(agent_types):
                rag_agent = [r for r in rag_results if r['agent_type'] == agent_type]
                no_rag_agent = [r for r in no_rag_results if r['agent_type'] == agent_type]

                rag_agent_metrics = calc_metrics_from_sum(rag_agent)
                no_rag_agent_metrics = calc_metrics_from_sum(no_rag_agent)
                agent_improvement = ((rag_agent_metrics['f1_score'] - no_rag_agent_metrics['f1_score']) / no_rag_agent_metrics['f1_score'] * 100) if no_rag_agent_metrics['f1_score'] > 0 else 0

                print(f"   {agent_type:20s}: RAG F1={rag_agent_metrics['f1_score']:.3f} "
                      f"(TP={rag_agent_metrics['tp']}, FP={rag_agent_metrics['fp']}, FN={rag_agent_metrics['fn']}, ì—ëŸ¬={rag_agent_metrics['error_count']}), "
                      f"ìˆœìˆ˜ F1={no_rag_agent_metrics['f1_score']:.3f} "
                      f"(TP={no_rag_agent_metrics['tp']}, FP={no_rag_agent_metrics['fp']}, FN={no_rag_agent_metrics['fn']}, ì—ëŸ¬={no_rag_agent_metrics['error_count']}), "
                      f"íš¨ê³¼={agent_improvement:+.1f}%")

        print("\n" + "=" * 80)


def main():
    parser = argparse.ArgumentParser(
        description="RAG íš¨ê³¼ ì¸¡ì • ë²¤ì¹˜ë§ˆí¬: ê°™ì€ ëª¨ë¸ë¡œ RAG ìœ ë¬´ ë¹„êµ"
    )
    parser.add_argument(
        '--models',
        nargs='+',
        default=['llama3:8b', 'gemini-2.0-flash-exp'],
        help='í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ (ê¸°ë³¸: llama3:8b gemini-2.0-flash-exp)'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=None,
        help='ì—ì´ì „íŠ¸ë‹¹ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìˆ˜ ì œí•œ'
    )
    parser.add_argument(
        '--agents',
        nargs='+',
        choices=['source_code', 'assembly_binary', 'logs_config'],
        default=None,
        help='í…ŒìŠ¤íŠ¸í•  ì—ì´ì „íŠ¸ ì„ íƒ'
    )
    parser.add_argument(
        '--pqc-url',
        default='http://localhost:8000',
        help='PQC Inspector ì„œë²„ URL'
    )
    parser.add_argument(
        '--ollama-url',
        default='http://localhost:11434',
        help='Ollama ì„œë²„ URL'
    )
    parser.add_argument(
        '--output',
        default=None,
        help='ê²°ê³¼ íŒŒì¼ ì´ë¦„'
    )
    parser.add_argument(
        '--auto-restart',
        action='store_true',
        help='AI-Server ìë™ ì¬ì‹œì‘ (ì‹¤í—˜ì )'
    )

    args = parser.parse_args()

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = RAGEffectBenchmark(
        pqc_base_url=args.pqc_url,
        ollama_base_url=args.ollama_url
    )

    benchmark.setup_test_models(args.models)

    benchmark.run_benchmark(
        limit_per_agent=args.limit,
        agent_filter=args.agents,
        auto_restart=args.auto_restart
    )

    # ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
    benchmark.print_summary()
    benchmark.save_results(args.output)


if __name__ == "__main__":
    main()
