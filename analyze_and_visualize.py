#!/usr/bin/env python3
"""
í†µí•© ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬
ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì‹œê°í™”í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python analyze_and_visualize.py <results_file> [options]

ì˜µì…˜:
    --all                ëª¨ë“  ë¶„ì„ ë° ì‹œê°í™” ìˆ˜í–‰ (ê¸°ë³¸ê°’)
    --text-only          í…ìŠ¤íŠ¸ ë¶„ì„ë§Œ ìˆ˜í–‰
    --visualize-only     ì‹œê°í™”ë§Œ ìˆ˜í–‰
    --output-dir DIR     ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: result_analysis)
    --min-tests N        ìµœì†Œ í…ŒìŠ¤íŠ¸ ìˆ˜ (ê¸°ë³¸ê°’: 10)
"""

import json
import argparse
import os
import sys
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
matplotlib.use('Agg')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['AppleGothic', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class ComprehensiveAnalyzer:
    """í†µí•© ë¶„ì„ ë° ì‹œê°í™” í´ë˜ìŠ¤"""

    ALGORITHM_VARIATIONS = {
        'rsa': ['rsa'],
        'ecc': ['ecc', 'ecdsa', 'ecdh', 'elliptic curve', 'p-256', 'secp256'],
        'aes': ['aes', 'rijndael', 'aes-128', 'aes-256'],
        'md5': ['md5', 'message digest 5'],
        'seed': ['seed'],
        'aria': ['aria'],
        'hight': ['hight'],
        'lea': ['lea'],
        'sha1': ['sha1', 'sha-1'],
        'sha256': ['sha256', 'sha-256'],
        '3des': ['3des', 'triple des', 'triple-des', 'tdes'],
        'des': ['des'],
        'rc4': ['rc4', 'arcfour'],
        'dsa': ['dsa'],
        'dh': ['diffie-hellman', 'dh', 'diffie hellman'],
        'elgamal': ['elgamal'],
        'kcdsa': ['kcdsa', 'ec-kcdsa', 'eckcdsa'],
        'has-160': ['has-160', 'has160'],
        'lsh': ['lsh'],
    }

    def __init__(self, results_file: str, output_dir: str = "result_analysis", min_tests: int = 10):
        self.results_file = results_file
        self.output_dir = output_dir
        self.min_tests = min_tests
        self.results = None
        self.df = None
        self.ground_truth_cache = {}

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(output_dir).mkdir(parents=True, exist_ok=True)

    def load_results(self):
        """ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.results_file, 'r', encoding='utf-8') as f:
                self.results = json.load(f)

            # í˜•ì‹ í†µì¼
            if 'detailed_results' in self.results and 'results' not in self.results:
                self.results['results'] = self.results['detailed_results']

            print(f"âœ… ê²°ê³¼ íŒŒì¼ ë¡œë“œ: {self.results_file}")
            print(f"   ì´ í…ŒìŠ¤íŠ¸: {self.results.get('metadata', {}).get('total_tests', 0)}ê°œ")
            return True
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def create_dataframe(self):
        """ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        detailed_results = self.results.get('detailed_results', self.results.get('results', []))
        successful_results = [r for r in detailed_results if r.get('success', False)]

        if not successful_results:
            print("âš ï¸  ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        self.df = pd.DataFrame(successful_results)
        self.df['provider_model'] = self.df['provider'] + '/' + self.df['model']

        # Precision, Recall, F1 ê³„ì‚°
        self._calculate_metrics()

        print(f"âœ… DataFrame ìƒì„± ì™„ë£Œ: {len(self.df)}ê°œ ì„±ê³µ í…ŒìŠ¤íŠ¸")
        return True

    def _load_ground_truth(self, file_path: str):
        """Ground truth ë¡œë“œ"""
        if file_path in self.ground_truth_cache:
            return self.ground_truth_cache[file_path]

        path = Path(file_path)
        if 'test_files' in path.parts:
            parts = list(path.parts)
            test_files_idx = parts.index('test_files')
            parts[test_files_idx] = 'ground_truth'
            gt_path = Path(*parts).with_suffix('.json')

            try:
                with open(gt_path, 'r', encoding='utf-8') as f:
                    gt_data = json.load(f)
                    self.ground_truth_cache[file_path] = gt_data
                    return gt_data
            except:
                return None
        return None

    def _normalize_algorithm_name(self, name: str) -> str:
        """ì•Œê³ ë¦¬ì¦˜ ì´ë¦„ ì •ê·œí™”"""
        name_lower = name.lower().strip()

        for standard_name, variations in self.ALGORITHM_VARIATIONS.items():
            if any(var in name_lower for var in variations):
                return standard_name.upper()

        return name.upper()

    def _calculate_precision_recall(self, detected: list, expected: list) -> tuple:
        """Precision, Recall, F1 ê³„ì‚°"""
        if not expected:
            return 1.0, 1.0, 1.0

        if not detected:
            return 0.0, 0.0, 0.0

        # ì •ê·œí™”
        detected_set = set(self._normalize_algorithm_name(alg) for alg in detected)
        expected_set = set(self._normalize_algorithm_name(alg) for alg in expected)

        # True Positives
        true_positives = len(detected_set & expected_set)

        # Precision, Recall, F1
        precision = true_positives / len(detected_set) if detected_set else 0.0
        recall = true_positives / len(expected_set) if expected_set else 0.0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

        return precision, recall, f1

    def _calculate_metrics(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ì— ëŒ€í•´ Precision, Recall, F1 ê³„ì‚°"""
        precisions = []
        recalls = []
        f1_scores = []

        for idx, row in self.df.iterrows():
            file_path = row.get('file_path', '')
            detected_algos = row.get('detected_algorithms', [])

            gt = self._load_ground_truth(file_path)

            if gt and 'expected_findings' in gt:
                expected_algos = gt['expected_findings'].get('vulnerable_algorithms_detected', [])
                expected_algos += gt['expected_findings'].get('korean_algorithms_detected', [])
                precision, recall, f1 = self._calculate_precision_recall(detected_algos, expected_algos)
            else:
                # Ground truthê°€ ì—†ìœ¼ë©´ confidence_score ì‚¬ìš©
                precision = recall = f1 = row.get('confidence_score', 0.0)

            precisions.append(precision)
            recalls.append(recall)
            f1_scores.append(f1)

        self.df['precision'] = precisions
        self.df['recall'] = recalls
        self.df['f1_score'] = f1_scores

        print(f"âœ… ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ (í‰ê·  F1: {np.mean(f1_scores):.3f})")

    # ==================== í…ìŠ¤íŠ¸ ë¶„ì„ ====================

    def generate_text_report(self):
        """ì¢…í•© í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        output_file = f"{self.output_dir}/COMPREHENSIVE_REPORT.txt"

        with open(output_file, 'w', encoding='utf-8') as f:
            self._write_header(f)
            self._write_summary(f)
            self._write_model_comparison(f)
            self._write_agent_analysis(f)
            self._write_algorithm_analysis(f)
            self._write_performance_analysis(f)

        print(f"âœ… í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±: {output_file}")

    def _write_header(self, f):
        """í—¤ë” ì‘ì„±"""
        f.write("=" * 100 + "\n")
        f.write("ğŸ“Š AI ë²¤ì¹˜ë§ˆí¬ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ\n")
        f.write("   ì–‘ì ì·¨ì•½ ì•”í˜¸ ì•Œê³ ë¦¬ì¦˜ íƒì§€ ì„±ëŠ¥ í‰ê°€\n")
        f.write("=" * 100 + "\n\n")
        f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ê²°ê³¼ íŒŒì¼: {self.results_file}\n")
        f.write(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}\n\n")

    def _write_summary(self, f):
        """ìš”ì•½ ì •ë³´ ì‘ì„±"""
        f.write("## 1ï¸âƒ£ ì‹¤í–‰ ìš”ì•½\n")
        f.write("-" * 100 + "\n")

        summary = self.results.get('summary', {})
        f.write(f"ì´ í…ŒìŠ¤íŠ¸: {summary.get('total_tests', 0)}ê°œ\n")
        f.write(f"ì„±ê³µ í…ŒìŠ¤íŠ¸: {summary.get('successful_tests', 0)}ê°œ\n")
        f.write(f"ì„±ê³µë¥ : {summary.get('success_rate', 0) * 100:.1f}%\n")
        f.write(f"í‰ê·  ì‹ ë¢°ë„: {summary.get('avg_confidence', 0):.3f}\n")
        f.write(f"í‰ê·  ì‘ë‹µì‹œê°„: {summary.get('avg_response_time', 0):.2f}ì´ˆ\n\n")

    def _write_model_comparison(self, f):
        """ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì‘ì„±"""
        f.write("## 2ï¸âƒ£ ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ (F1 Score ê¸°ì¤€)\n")
        f.write("=" * 100 + "\n\n")

        model_stats = self.df.groupby('provider_model').agg({
            'f1_score': ['mean', 'std', 'min', 'max', 'count'],
            'precision': ['mean', 'std'],
            'recall': ['mean', 'std'],
            'response_time': ['mean', 'std'],
            'confidence_score': 'mean'
        }).round(3)

        model_stats.columns = ['_'.join(col).strip('_') for col in model_stats.columns]
        model_stats = model_stats.sort_values('f1_score_mean', ascending=False)

        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ (min_tests ì´ìƒ)
        reliable = model_stats[model_stats['f1_score_count'] >= self.min_tests]

        if not reliable.empty:
            f.write(f"ğŸ† ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ (í…ŒìŠ¤íŠ¸ ìˆ˜ >= {self.min_tests}):\n")
            f.write("-" * 100 + "\n")

            for i, (model, stats) in enumerate(reliable.iterrows(), 1):
                medal = 'ğŸ¥‡' if i == 1 else 'ğŸ¥ˆ' if i == 2 else 'ğŸ¥‰' if i == 3 else '  '
                f.write(f"{medal} {i}. {model}\n")
                f.write(f"   F1 Score: {stats['f1_score_mean']:.3f} (Â±{stats['f1_score_std']:.3f})\n")
                f.write(f"   Precision: {stats['precision_mean']:.3f} (Â±{stats['precision_std']:.3f})\n")
                f.write(f"   Recall: {stats['recall_mean']:.3f} (Â±{stats['recall_std']:.3f})\n")
                f.write(f"   ì‘ë‹µì‹œê°„: {stats['response_time_mean']:.2f}ì´ˆ (Â±{stats['response_time_std']:.2f})\n")
                f.write(f"   í…ŒìŠ¤íŠ¸ ìˆ˜: {int(stats['f1_score_count'])}ê°œ\n\n")

        # ì „ì²´ ê²°ê³¼
        f.write(f"\nì „ì²´ ëª¨ë¸ ìˆœìœ„ (ëª¨ë“  í…ŒìŠ¤íŠ¸):\n")
        f.write("-" * 100 + "\n")
        for i, (model, stats) in enumerate(model_stats.iterrows(), 1):
            warning = " âš ï¸ " if stats['f1_score_count'] < self.min_tests else ""
            f.write(f"  {i}. {model}: F1 {stats['f1_score_mean']:.3f} ({int(stats['f1_score_count'])}ê°œ){warning}\n")
        f.write("\n")

    def _write_agent_analysis(self, f):
        """ì—ì´ì „íŠ¸ë³„ ë¶„ì„ ì‘ì„±"""
        f.write("## 3ï¸âƒ£ ì—ì´ì „íŠ¸ë³„ ì„±ëŠ¥ ë¶„ì„\n")
        f.write("=" * 100 + "\n\n")

        agent_stats = self.df.groupby('agent_type').agg({
            'f1_score': ['mean', 'std', 'count'],
            'precision': 'mean',
            'recall': 'mean',
            'response_time': 'mean',
            'confidence_score': 'mean'
        }).round(3)

        agent_stats.columns = ['_'.join(col).strip('_') for col in agent_stats.columns]

        for agent, stats in agent_stats.iterrows():
            f.write(f"ğŸ¯ {agent}\n")
            f.write("-" * 100 + "\n")
            f.write(f"  í…ŒìŠ¤íŠ¸ ìˆ˜: {int(stats['f1_score_count'])}ê°œ\n")
            f.write(f"  F1 Score: {stats['f1_score_mean']:.3f} (Â±{stats['f1_score_std']:.3f})\n")
            f.write(f"  Precision: {stats['precision_mean']:.3f}\n")
            f.write(f"  Recall: {stats['recall_mean']:.3f}\n")
            f.write(f"  í‰ê·  ì‘ë‹µì‹œê°„: {stats['response_time_mean']:.2f}ì´ˆ\n\n")

    def _write_algorithm_analysis(self, f):
        """ì•Œê³ ë¦¬ì¦˜ë³„ íƒì§€ ë¶„ì„ ì‘ì„±"""
        f.write("## 4ï¸âƒ£ ì•Œê³ ë¦¬ì¦˜ íƒì§€ ë¶„ì„\n")
        f.write("=" * 100 + "\n\n")

        algorithm_stats = self._collect_algorithm_stats()

        if algorithm_stats:
            f.write("ì „ì²´ ì•Œê³ ë¦¬ì¦˜ íƒì§€ìœ¨:\n")
            f.write("-" * 100 + "\n")

            for algo, stats in sorted(algorithm_stats.items(),
                                     key=lambda x: x[1]['detected'] / max(x[1]['expected'], 1),
                                     reverse=True):
                rate = stats['detected'] / max(stats['expected'], 1) * 100
                status = "âœ…" if rate >= 80 else "âš ï¸ " if rate >= 50 else "âŒ"
                f.write(f"  {status} {algo:15s}: {rate:5.1f}% ({stats['detected']}/{stats['expected']})\n")
            f.write("\n")

    def _write_performance_analysis(self, f):
        """ì„±ëŠ¥ ë¶„ì„ ì‘ì„±"""
        f.write("## 5ï¸âƒ£ ì„±ëŠ¥ ë¶„ì„\n")
        f.write("=" * 100 + "\n\n")

        # ì‘ë‹µì‹œê°„
        f.write("âš¡ ì‘ë‹µì‹œê°„ ìˆœìœ„:\n")
        f.write("-" * 100 + "\n")
        response_stats = self.df.groupby('provider_model')['response_time'].mean().sort_values()
        for i, (model, time) in enumerate(response_stats.items(), 1):
            f.write(f"  {i}. {model}: {time:.2f}ì´ˆ\n")
        f.write("\n")

        # ìƒê´€ê´€ê³„
        if len(self.df) >= 10:
            f.write("ğŸ”— ì£¼ìš” ìƒê´€ê´€ê³„:\n")
            f.write("-" * 100 + "\n")
            corr = self.df[['f1_score', 'response_time', 'confidence_score']].corr()
            f.write(f"  F1 Score vs ì‘ë‹µì‹œê°„: {corr.loc['f1_score', 'response_time']:.3f}\n")
            f.write(f"  F1 Score vs ì‹ ë¢°ë„: {corr.loc['f1_score', 'confidence_score']:.3f}\n\n")

    def _collect_algorithm_stats(self):
        """ì•Œê³ ë¦¬ì¦˜ í†µê³„ ìˆ˜ì§‘"""
        stats = defaultdict(lambda: {'expected': 0, 'detected': 0})

        for idx, row in self.df.iterrows():
            file_path = row.get('file_path', '')
            detected_algos = row.get('detected_algorithms', [])

            gt = self._load_ground_truth(file_path)
            if not gt or 'expected_findings' not in gt:
                continue

            expected_algos = gt['expected_findings'].get('vulnerable_algorithms_detected', [])
            expected_algos += gt['expected_findings'].get('korean_algorithms_detected', [])

            for algo in expected_algos:
                norm_algo = self._normalize_algorithm_name(algo)
                stats[norm_algo]['expected'] += 1

                # íƒì§€ ì—¬ë¶€ í™•ì¸
                detected_norm = [self._normalize_algorithm_name(d) for d in detected_algos]
                if norm_algo in detected_norm:
                    stats[norm_algo]['detected'] += 1

        return dict(stats)

    # ==================== ì‹œê°í™” ====================

    def generate_visualizations(self):
        """ëª¨ë“  ì‹œê°í™” ìƒì„±"""
        print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")

        self._plot_model_f1_comparison()
        self._plot_precision_recall_f1()
        self._plot_agent_performance()
        self._plot_response_time()
        self._plot_algorithm_detection()
        self._plot_model_heatmap()

        print("âœ… ëª¨ë“  ì‹œê°í™” ìƒì„± ì™„ë£Œ")

    def _plot_model_f1_comparison(self):
        """ëª¨ë¸ë³„ F1 Score ë¹„êµ"""
        model_stats = self.df.groupby('provider_model').agg({
            'f1_score': ['mean', 'count']
        })
        model_stats.columns = ['mean', 'count']
        model_stats = model_stats[model_stats['count'] >= self.min_tests].sort_values('mean', ascending=True)

        if model_stats.empty:
            return

        fig, ax = plt.subplots(figsize=(12, max(6, len(model_stats) * 0.4)))

        colors = ['#2ecc71' if x >= 0.8 else '#f39c12' if x >= 0.5 else '#e74c3c'
                 for x in model_stats['mean']]
        bars = ax.barh(range(len(model_stats)), model_stats['mean'], color=colors, alpha=0.8)

        # ê°’ í‘œì‹œ
        for i, (bar, f1, count) in enumerate(zip(bars, model_stats['mean'], model_stats['count'])):
            ax.text(f1 + 0.01, i, f'{f1:.3f} (n={int(count)})',
                   va='center', fontsize=9, fontweight='bold')

        ax.set_yticks(range(len(model_stats)))
        ax.set_yticklabels(model_stats.index, fontsize=10)
        ax.set_xlabel('F1 Score', fontsize=12, fontweight='bold')
        ax.set_title('Model Performance Comparison (F1 Score)',
                    fontsize=14, fontweight='bold', pad=20)
        ax.set_xlim(0, min(1.0, max(model_stats['mean']) * 1.15))
        ax.grid(axis='x', alpha=0.3, linestyle='--')

        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/model_f1_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  âœ“ model_f1_comparison.png")

    def _plot_precision_recall_f1(self):
        """Precision, Recall, F1 í•¨ê»˜ ë¹„êµ"""
        model_stats = self.df.groupby('provider_model').agg({
            'precision': 'mean',
            'recall': 'mean',
            'f1_score': ['mean', 'count']
        })
        model_stats.columns = ['precision', 'recall', 'f1', 'count']
        model_stats = model_stats[model_stats['count'] >= self.min_tests].sort_values('f1', ascending=False)

        if model_stats.empty:
            return

        x = np.arange(len(model_stats))
        width = 0.25

        fig, ax = plt.subplots(figsize=(14, 7))

        ax.bar(x - width, model_stats['precision'], width, label='Precision',
               color='#3498db', edgecolor='black', alpha=0.8)
        ax.bar(x, model_stats['recall'], width, label='Recall',
               color='#e74c3c', edgecolor='black', alpha=0.8)
        ax.bar(x + width, model_stats['f1'], width, label='F1 Score',
               color='#2ecc71', edgecolor='black', alpha=0.8)

        ax.set_ylabel('Score', fontsize=12, fontweight='bold')
        ax.set_title('Precision, Recall, and F1 Score Comparison',
                    fontsize=14, fontweight='bold', pad=20)
        ax.set_xticks(x)
        ax.set_xticklabels(model_stats.index, rotation=45, ha='right', fontsize=9)
        ax.legend(fontsize=11)
        ax.grid(axis='y', alpha=0.3, linestyle='--')
        ax.set_ylim(0, 1.1)

        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/precision_recall_f1.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  âœ“ precision_recall_f1.png")

    def _plot_agent_performance(self):
        """ì—ì´ì „íŠ¸ë³„ ì„±ëŠ¥"""
        agent_stats = self.df.groupby('agent_type').agg({
            'f1_score': 'mean',
            'confidence_score': 'count'
        })
        agent_stats.columns = ['f1', 'count']
        agent_stats = agent_stats.sort_values('f1', ascending=True)

        fig, ax = plt.subplots(figsize=(10, 6))

        colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(agent_stats)))
        bars = ax.barh(range(len(agent_stats)), agent_stats['f1'], color=colors, alpha=0.8)

        for i, (bar, f1, count) in enumerate(zip(bars, agent_stats['f1'], agent_stats['count'])):
            ax.text(f1 + 0.01, i, f'{f1:.3f} (n={int(count)})',
                   va='center', fontsize=10, fontweight='bold')

        ax.set_yticks(range(len(agent_stats)))
        ax.set_yticklabels(agent_stats.index, fontsize=11)
        ax.set_xlabel('F1 Score', fontsize=12, fontweight='bold')
        ax.set_title('Agent Performance Comparison', fontsize=14, fontweight='bold', pad=20)
        ax.set_xlim(0, min(1.0, max(agent_stats['f1']) * 1.15))
        ax.grid(axis='x', alpha=0.3, linestyle='--')

        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/agent_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  âœ“ agent_performance.png")

    def _plot_response_time(self):
        """ì‘ë‹µì‹œê°„ ë¹„êµ"""
        response_stats = self.df.groupby('provider_model').agg({
            'response_time': ['mean', 'std', 'count']
        })
        response_stats.columns = ['mean', 'std', 'count']
        response_stats = response_stats[response_stats['count'] >= self.min_tests].sort_values('mean')

        if response_stats.empty:
            return

        fig, ax = plt.subplots(figsize=(12, max(6, len(response_stats) * 0.4)))

        max_time = response_stats['mean'].max()
        colors = plt.cm.RdYlGn_r(response_stats['mean'] / max_time)

        bars = ax.barh(range(len(response_stats)), response_stats['mean'],
                      xerr=response_stats['std'], capsize=5,
                      color=colors, edgecolor='black', alpha=0.8)

        for i, (bar, time, count) in enumerate(zip(bars, response_stats['mean'], response_stats['count'])):
            ax.text(time + response_stats['std'].iloc[i] + 0.5, i,
                   f'{time:.2f}s (n={int(count)})',
                   va='center', fontsize=9, fontweight='bold')

        ax.set_yticks(range(len(response_stats)))
        ax.set_yticklabels(response_stats.index, fontsize=10)
        ax.set_xlabel('Response Time (seconds)', fontsize=12, fontweight='bold')
        ax.set_title('Model Response Time Comparison', fontsize=14, fontweight='bold', pad=20)
        ax.grid(axis='x', alpha=0.3, linestyle='--')

        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/model_response_time.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  âœ“ model_response_time.png")

    def _plot_algorithm_detection(self):
        """ì•Œê³ ë¦¬ì¦˜ íƒì§€ìœ¨"""
        algorithm_stats = self._collect_algorithm_stats()

        if not algorithm_stats:
            return

        # íƒì§€ìœ¨ ê³„ì‚° ë° ì •ë ¬
        algo_data = []
        for algo, stats in algorithm_stats.items():
            if stats['expected'] > 0:
                rate = stats['detected'] / stats['expected'] * 100
                algo_data.append((algo, rate, stats['expected']))

        algo_data.sort(key=lambda x: x[1], reverse=True)
        algorithms, rates, counts = zip(*algo_data)

        fig, ax = plt.subplots(figsize=(14, max(8, len(algorithms) * 0.4)))

        colors = ['#2ecc71' if r >= 80 else '#f39c12' if r >= 50 else '#e74c3c' for r in rates]
        bars = ax.barh(range(len(algorithms)), rates, color=colors, alpha=0.8)

        for i, (bar, rate, count) in enumerate(zip(bars, rates, counts)):
            ax.text(rate + 2, i, f'{rate:.1f}% (n={count})',
                   va='center', fontsize=9, fontweight='bold')

        ax.set_yticks(range(len(algorithms)))
        ax.set_yticklabels(algorithms, fontsize=10)
        ax.set_xlabel('Detection Rate (%)', fontsize=12, fontweight='bold')
        ax.set_title('Algorithm Detection Rate (All Models)',
                    fontsize=14, fontweight='bold', pad=20)
        ax.set_xlim(0, 110)
        ax.grid(axis='x', alpha=0.3, linestyle='--')

        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/algorithm_detection_overall.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  âœ“ algorithm_detection_overall.png")

    def _plot_model_heatmap(self):
        """ëª¨ë¸-ì—ì´ì „íŠ¸ íˆíŠ¸ë§µ"""
        pivot_data = self.df.groupby(['agent_type', 'provider_model']).agg({
            'f1_score': 'mean',
            'confidence_score': 'count'
        }).reset_index()

        pivot_data = pivot_data[pivot_data['confidence_score'] >= self.min_tests]

        if pivot_data.empty:
            return

        heatmap_data = pivot_data.pivot(
            index='agent_type',
            columns='provider_model',
            values='f1_score'
        )

        fig, ax = plt.subplots(figsize=(14, 8))

        sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlGnBu',
                   cbar_kws={'label': 'F1 Score'}, linewidths=0.5,
                   linecolor='gray', ax=ax)

        ax.set_title('Model-Agent Performance Heatmap (F1 Score)',
                    fontsize=14, fontweight='bold', pad=20)
        ax.set_xlabel('Model', fontsize=12, fontweight='bold')
        ax.set_ylabel('Agent Type', fontsize=12, fontweight='bold')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)

        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/model_agent_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  âœ“ model_agent_heatmap.png")

    # ==================== ë©”ì¸ ì‹¤í–‰ ====================

    def run_all(self):
        """ì „ì²´ ë¶„ì„ ë° ì‹œê°í™” ì‹¤í–‰"""
        print("\n" + "=" * 80)
        print("ğŸš€ í†µí•© ë¶„ì„ ë° ì‹œê°í™” ì‹œì‘")
        print("=" * 80)

        if not self.load_results():
            return False

        if not self.create_dataframe():
            return False

        print("\nğŸ“ í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        self.generate_text_report()

        print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        self.generate_visualizations()

        print("\n" + "=" * 80)
        print(f"âœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ëŠ” '{self.output_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("=" * 80)
        print("\nìƒì„±ëœ íŒŒì¼:")
        print("  ğŸ“„ COMPREHENSIVE_REPORT.txt - ì¢…í•© í…ìŠ¤íŠ¸ ë³´ê³ ì„œ")
        print("  ğŸ“Š model_f1_comparison.png - ëª¨ë¸ë³„ F1 Score ë¹„êµ")
        print("  ğŸ“Š precision_recall_f1.png - Precision/Recall/F1 ë¹„êµ")
        print("  ğŸ“Š agent_performance.png - ì—ì´ì „íŠ¸ë³„ ì„±ëŠ¥")
        print("  ğŸ“Š model_response_time.png - ëª¨ë¸ë³„ ì‘ë‹µì‹œê°„")
        print("  ğŸ“Š algorithm_detection_overall.png - ì•Œê³ ë¦¬ì¦˜ íƒì§€ìœ¨")
        print("  ğŸ“Š model_agent_heatmap.png - ëª¨ë¸-ì—ì´ì „íŠ¸ íˆíŠ¸ë§µ")
        print()

        return True


def main():
    parser = argparse.ArgumentParser(
        description='í†µí•© ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ì „ì²´ ë¶„ì„ ë° ì‹œê°í™”
  python analyze_and_visualize.py benchmark_results.json

  # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì§€ì •
  python analyze_and_visualize.py benchmark_results.json --output-dir my_results

  # ìµœì†Œ í…ŒìŠ¤íŠ¸ ìˆ˜ ì„¤ì •
  python analyze_and_visualize.py benchmark_results.json --min-tests 20

  # í…ìŠ¤íŠ¸ ë¶„ì„ë§Œ
  python analyze_and_visualize.py benchmark_results.json --text-only

  # ì‹œê°í™”ë§Œ
  python analyze_and_visualize.py benchmark_results.json --visualize-only
        """
    )

    parser.add_argument('results_file', help='ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ JSON íŒŒì¼')
    parser.add_argument('--output-dir', default='result_analysis',
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: result_analysis)')
    parser.add_argument('--min-tests', type=int, default=10,
                       help='ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ë¡œ ê°„ì£¼í•˜ëŠ” ìµœì†Œ í…ŒìŠ¤íŠ¸ ìˆ˜ (ê¸°ë³¸ê°’: 10)')
    parser.add_argument('--text-only', action='store_true',
                       help='í…ìŠ¤íŠ¸ ë¶„ì„ë§Œ ìˆ˜í–‰')
    parser.add_argument('--visualize-only', action='store_true',
                       help='ì‹œê°í™”ë§Œ ìˆ˜í–‰')
    parser.add_argument('--all', action='store_true', default=True,
                       help='ëª¨ë“  ë¶„ì„ ë° ì‹œê°í™” ìˆ˜í–‰ (ê¸°ë³¸ê°’)')

    args = parser.parse_args()

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(args.results_file).exists():
        print(f"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.results_file}")
        sys.exit(1)

    # ë¶„ì„ê¸° ìƒì„±
    analyzer = ComprehensiveAnalyzer(
        results_file=args.results_file,
        output_dir=args.output_dir,
        min_tests=args.min_tests
    )

    # ê²°ê³¼ ë¡œë“œ ë° DataFrame ìƒì„±
    if not analyzer.load_results() or not analyzer.create_dataframe():
        sys.exit(1)

    # ë¶„ì„ ì‹¤í–‰
    if args.text_only:
        print("\nğŸ“ í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        analyzer.generate_text_report()
    elif args.visualize_only:
        analyzer.generate_visualizations()
    else:
        analyzer.run_all()


if __name__ == "__main__":
    main()
