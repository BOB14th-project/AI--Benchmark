#!/usr/bin/env python3
"""
ì—ì´ì „íŠ¸ë³„ ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™” ë„êµ¬
ê° ì—ì´ì „íŠ¸ì—ì„œ ì–´ë–¤ ëª¨ë¸ì´ ê°€ì¥ ë›°ì–´ë‚œì§€ ë¹„êµ ë¶„ì„
"""

import json
import argparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path

class AgentPerformanceVisualizer:
    def __init__(self, results_file: str):
        self.results_file = results_file
        self.results = self._load_results()
        self.df = self._create_dataframe()

    def _load_results(self):
        """ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        with open(self.results_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _create_dataframe(self):
        """ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        detailed_results = self.results.get('detailed_results', [])
        successful_results = [r for r in detailed_results if r.get('success', False)]

        if not successful_results:
            print("âŒ ë¶„ì„í•  ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        df = pd.DataFrame(successful_results)
        df['provider_model'] = df['provider'] + '/' + df['model']

        # F1, Precision, Recallì´ ì´ë¯¸ ìˆë‹¤ê³  ê°€ì •
        # ì—†ìœ¼ë©´ confidence_score ì‚¬ìš©
        if 'f1_score' not in df.columns:
            df['f1_score'] = df['confidence_score']
        if 'precision' not in df.columns:
            df['precision'] = df['confidence_score']
        if 'recall' not in df.columns:
            df['recall'] = df['confidence_score']

        return df

    def create_agent_model_heatmap(self, metric='f1_score', min_tests=10):
        """ì—ì´ì „íŠ¸-ëª¨ë¸ë³„ ì„±ëŠ¥ íˆíŠ¸ë§µ ìƒì„±"""
        if self.df.empty:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì—ì´ì „íŠ¸-ëª¨ë¸ë³„ ì§‘ê³„
        pivot_data = self.df.groupby(['agent_type', 'provider_model']).agg({
            metric: 'mean',
            'confidence_score': 'count'
        }).reset_index()

        # ìµœì†Œ í…ŒìŠ¤íŠ¸ ìˆ˜ í•„í„°ë§
        pivot_data = pivot_data[pivot_data['confidence_score'] >= min_tests]

        if pivot_data.empty:
            print(f"âŒ ìµœì†Œ {min_tests}ê°œ í…ŒìŠ¤íŠ¸ë¥¼ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # í”¼ë²— í…Œì´ë¸” ìƒì„±
        heatmap_data = pivot_data.pivot(
            index='agent_type',
            columns='provider_model',
            values=metric
        )

        # ê·¸ë˜í”„ ìƒì„±
        plt.figure(figsize=(14, 8))
        sns.heatmap(
            heatmap_data,
            annot=True,
            fmt='.3f',
            cmap='YlOrRd',
            cbar_kws={'label': metric.replace('_', ' ').title()},
            linewidths=0.5,
            linecolor='gray'
        )

        plt.title(f'Agent-Model Performance Heatmap ({metric.replace("_", " ").title()})',
                  fontsize=16, fontweight='bold', pad=20)
        plt.xlabel('Model', fontsize=12, fontweight='bold')
        plt.ylabel('Agent Type', fontsize=12, fontweight='bold')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()

        filename = f'agent_model_heatmap_{metric}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š íˆíŠ¸ë§µì´ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        plt.close()

    def create_agent_comparison_bar(self, min_tests=10):
        """ì—ì´ì „íŠ¸ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¹„êµ ë§‰ëŒ€ ê·¸ë˜í”„"""
        if self.df.empty:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì—ì´ì „íŠ¸-ëª¨ë¸ë³„ ì§‘ê³„
        agent_model_stats = self.df.groupby(['agent_type', 'provider_model']).agg({
            'f1_score': 'mean',
            'precision': 'mean',
            'recall': 'mean',
            'confidence_score': 'count'
        }).reset_index()

        # ìµœì†Œ í…ŒìŠ¤íŠ¸ ìˆ˜ í•„í„°ë§
        agent_model_stats = agent_model_stats[agent_model_stats['confidence_score'] >= min_tests]

        if agent_model_stats.empty:
            print(f"âŒ ìµœì†Œ {min_tests}ê°œ í…ŒìŠ¤íŠ¸ë¥¼ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ê° ì—ì´ì „íŠ¸ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        best_models = agent_model_stats.loc[
            agent_model_stats.groupby('agent_type')['f1_score'].idxmax()
        ]

        # ê·¸ë˜í”„ ìƒì„±
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        metrics = ['f1_score', 'precision', 'recall']
        titles = ['F1 Score', 'Precision', 'Recall']
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']

        for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
            ax = axes[idx]

            # ë§‰ëŒ€ ê·¸ë˜í”„
            bars = ax.barh(
                range(len(best_models)),
                best_models[metric],
                color=color,
                alpha=0.7,
                edgecolor='black',
                linewidth=1.5
            )

            # ë ˆì´ë¸” ì„¤ì •
            ax.set_yticks(range(len(best_models)))
            ax.set_yticklabels(best_models['agent_type'])
            ax.set_xlabel(title, fontsize=11, fontweight='bold')
            ax.set_title(f'Best Model per Agent ({title})', fontsize=12, fontweight='bold')
            ax.set_xlim(0, 1.0)
            ax.grid(axis='x', alpha=0.3, linestyle='--')

            # ê°’ê³¼ ëª¨ë¸ëª… í‘œì‹œ
            for i, (bar, model, value) in enumerate(zip(bars, best_models['provider_model'], best_models[metric])):
                # ê°’ í‘œì‹œ
                ax.text(
                    value + 0.02,
                    i,
                    f'{value:.3f}',
                    va='center',
                    fontsize=9,
                    fontweight='bold'
                )
                # ëª¨ë¸ëª… í‘œì‹œ (ë§‰ëŒ€ ì•ˆìª½)
                ax.text(
                    value / 2,
                    i,
                    model.split('/')[-1],  # ëª¨ë¸ëª…ë§Œ í‘œì‹œ
                    va='center',
                    ha='center',
                    fontsize=8,
                    color='white',
                    fontweight='bold'
                )

        plt.suptitle('Best Performing Model for Each Agent Type',
                     fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()

        filename = 'agent_best_models.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ë§‰ëŒ€ ê·¸ë˜í”„ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        plt.close()

    def create_agent_model_ranking(self, min_tests=10):
        """ì—ì´ì „íŠ¸ë³„ ëª¨ë¸ ìˆœìœ„ ì°¨íŠ¸"""
        if self.df.empty:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì—ì´ì „íŠ¸-ëª¨ë¸ë³„ ì§‘ê³„
        agent_model_stats = self.df.groupby(['agent_type', 'provider_model']).agg({
            'f1_score': 'mean',
            'confidence_score': 'count'
        }).reset_index()

        # ìµœì†Œ í…ŒìŠ¤íŠ¸ ìˆ˜ í•„í„°ë§
        agent_model_stats = agent_model_stats[agent_model_stats['confidence_score'] >= min_tests]

        agents = agent_model_stats['agent_type'].unique()
        n_agents = len(agents)

        # ê·¸ë˜í”„ ìƒì„± (ê° ì—ì´ì „íŠ¸ë³„ ì„œë¸Œí”Œë¡¯)
        fig, axes = plt.subplots(1, n_agents, figsize=(6*n_agents, 6))
        if n_agents == 1:
            axes = [axes]

        for idx, agent in enumerate(agents):
            ax = axes[idx]

            # í•´ë‹¹ ì—ì´ì „íŠ¸ ë°ì´í„° í•„í„°ë§
            agent_data = agent_model_stats[agent_model_stats['agent_type'] == agent].copy()
            agent_data = agent_data.sort_values('f1_score', ascending=True)

            # ë§‰ëŒ€ ê·¸ë˜í”„
            colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(agent_data)))
            bars = ax.barh(
                range(len(agent_data)),
                agent_data['f1_score'],
                color=colors,
                edgecolor='black',
                linewidth=1.5
            )

            # ë ˆì´ë¸”
            ax.set_yticks(range(len(agent_data)))
            model_labels = [m.split('/')[-1] for m in agent_data['provider_model']]
            ax.set_yticklabels(model_labels, fontsize=10)
            ax.set_xlabel('F1 Score', fontsize=11, fontweight='bold')
            ax.set_title(f'{agent}\nModel Rankings', fontsize=12, fontweight='bold')
            ax.set_xlim(0, max(agent_data['f1_score']) * 1.2)
            ax.grid(axis='x', alpha=0.3, linestyle='--')

            # ê°’ê³¼ ìˆœìœ„ í‘œì‹œ
            for i, (bar, f1, count) in enumerate(zip(bars, agent_data['f1_score'], agent_data['confidence_score'])):
                # F1 ì ìˆ˜
                ax.text(
                    f1 + 0.01,
                    i,
                    f'{f1:.3f}',
                    va='center',
                    fontsize=9,
                    fontweight='bold'
                )
                # í…ŒìŠ¤íŠ¸ ìˆ˜
                ax.text(
                    f1 / 2,
                    i,
                    f'n={int(count)}',
                    va='center',
                    ha='center',
                    fontsize=8,
                    color='white',
                    fontweight='bold'
                )

        plt.suptitle('Model Performance Rankings by Agent Type',
                     fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()

        filename = 'agent_model_rankings.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ìˆœìœ„ ì°¨íŠ¸ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        plt.close()

    def create_comprehensive_comparison(self, min_tests=10):
        """ì¢…í•© ë¹„êµ ê·¸ë˜í”„ (F1, Precision, Recall í•œëˆˆì—)"""
        if self.df.empty:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì—ì´ì „íŠ¸-ëª¨ë¸ë³„ ì§‘ê³„
        agent_model_stats = self.df.groupby(['agent_type', 'provider_model']).agg({
            'f1_score': 'mean',
            'precision': 'mean',
            'recall': 'mean',
            'confidence_score': 'count'
        }).reset_index()

        # ìµœì†Œ í…ŒìŠ¤íŠ¸ ìˆ˜ í•„í„°ë§
        agent_model_stats = agent_model_stats[agent_model_stats['confidence_score'] >= min_tests]

        agents = sorted(agent_model_stats['agent_type'].unique())

        # ê·¸ë˜í”„ ìƒì„±
        fig, ax = plt.subplots(figsize=(16, 10))

        # ê° ì—ì´ì „íŠ¸ë³„ë¡œ ê·¸ë£¹í™”
        bar_width = 0.25
        x_positions = {}
        current_x = 0

        for agent_idx, agent in enumerate(agents):
            agent_data = agent_model_stats[agent_model_stats['agent_type'] == agent].copy()
            agent_data = agent_data.sort_values('f1_score', ascending=False)

            n_models = len(agent_data)
            x_base = np.arange(n_models) * 4 + current_x
            x_positions[agent] = (x_base[0], x_base[-1])

            # F1, Precision, Recall ë§‰ëŒ€
            ax.bar(x_base - bar_width, agent_data['f1_score'],
                   bar_width, label='F1' if agent_idx == 0 else '',
                   color='#FF6B6B', alpha=0.8, edgecolor='black')
            ax.bar(x_base, agent_data['precision'],
                   bar_width, label='Precision' if agent_idx == 0 else '',
                   color='#4ECDC4', alpha=0.8, edgecolor='black')
            ax.bar(x_base + bar_width, agent_data['recall'],
                   bar_width, label='Recall' if agent_idx == 0 else '',
                   color='#45B7D1', alpha=0.8, edgecolor='black')

            # ëª¨ë¸ëª… í‘œì‹œ
            model_labels = [m.split('/')[-1][:15] for m in agent_data['provider_model']]
            ax.set_xticks(x_base)
            ax.set_xticklabels(model_labels, rotation=45, ha='right', fontsize=9)

            current_x = x_base[-1] + 6

        # ì—ì´ì „íŠ¸ êµ¬ë¶„ì„  ë° ë¼ë²¨
        for agent in agents:
            x_start, x_end = x_positions[agent]
            ax.axvline(x_start - 2, color='gray', linestyle='--', alpha=0.5, linewidth=1)
            ax.text((x_start + x_end) / 2, 1.05, agent,
                   ha='center', va='bottom', fontsize=12, fontweight='bold',
                   transform=ax.get_xaxis_transform())

        ax.set_ylabel('Score', fontsize=12, fontweight='bold')
        ax.set_title('Comprehensive Agent-Model Performance Comparison',
                    fontsize=16, fontweight='bold', pad=20)
        ax.set_ylim(0, 1.1)
        ax.legend(loc='upper right', fontsize=11)
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        plt.tight_layout()

        filename = 'comprehensive_agent_model_comparison.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ì¢…í•© ë¹„êµ ê·¸ë˜í”„ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        plt.close()

    def print_best_models_summary(self, min_tests=10):
        """ì—ì´ì „íŠ¸ë³„ ìµœê³  ëª¨ë¸ ìš”ì•½ ì¶œë ¥"""
        if self.df.empty:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì—ì´ì „íŠ¸-ëª¨ë¸ë³„ ì§‘ê³„
        agent_model_stats = self.df.groupby(['agent_type', 'provider_model']).agg({
            'f1_score': 'mean',
            'precision': 'mean',
            'recall': 'mean',
            'confidence_score': ['mean', 'count'],
            'response_time': 'mean'
        }).reset_index()

        # ì»¬ëŸ¼ëª… ì •ë¦¬
        agent_model_stats.columns = ['agent_type', 'provider_model', 'f1_score',
                                     'precision', 'recall', 'confidence_score',
                                     'test_count', 'response_time']

        # ìµœì†Œ í…ŒìŠ¤íŠ¸ ìˆ˜ í•„í„°ë§
        agent_model_stats = agent_model_stats[agent_model_stats['test_count'] >= min_tests]

        if agent_model_stats.empty:
            print(f"âŒ ìµœì†Œ {min_tests}ê°œ í…ŒìŠ¤íŠ¸ë¥¼ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print("\n" + "=" * 80)
        print("ğŸ† ì—ì´ì „íŠ¸ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìš”ì•½")
        print("=" * 80)

        agents = sorted(agent_model_stats['agent_type'].unique())

        for agent in agents:
            agent_data = agent_model_stats[agent_model_stats['agent_type'] == agent]

            # F1 ê¸°ì¤€ ìµœê³  ëª¨ë¸
            best_f1 = agent_data.loc[agent_data['f1_score'].idxmax()]

            # Precision ê¸°ì¤€ ìµœê³  ëª¨ë¸
            best_precision = agent_data.loc[agent_data['precision'].idxmax()]

            # Recall ê¸°ì¤€ ìµœê³  ëª¨ë¸
            best_recall = agent_data.loc[agent_data['recall'].idxmax()]

            # ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸
            fastest = agent_data.loc[agent_data['response_time'].idxmin()]

            print(f"\nğŸ¯ {agent}")
            print("-" * 80)
            print(f"  ğŸ¥‡ ìµœê³  F1 ì ìˆ˜: {best_f1['provider_model']}")
            print(f"     - F1: {best_f1['f1_score']:.3f}, Precision: {best_f1['precision']:.3f}, Recall: {best_f1['recall']:.3f}")
            print(f"     - í…ŒìŠ¤íŠ¸ ìˆ˜: {int(best_f1['test_count'])}ê°œ, ì‘ë‹µì‹œê°„: {best_f1['response_time']:.2f}ì´ˆ")

            if best_precision['provider_model'] != best_f1['provider_model']:
                print(f"  ğŸ“Š ìµœê³  Precision: {best_precision['provider_model']} ({best_precision['precision']:.3f})")

            if best_recall['provider_model'] != best_f1['provider_model']:
                print(f"  ğŸ“ˆ ìµœê³  Recall: {best_recall['provider_model']} ({best_recall['recall']:.3f})")

            if fastest['provider_model'] != best_f1['provider_model']:
                print(f"  âš¡ ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸: {fastest['provider_model']} ({fastest['response_time']:.2f}ì´ˆ)")

            # ì „ì²´ ëª¨ë¸ ìˆœìœ„ (F1 ê¸°ì¤€)
            agent_data_sorted = agent_data.sort_values('f1_score', ascending=False)
            print(f"\n  ì „ì²´ ëª¨ë¸ ìˆœìœ„ (F1 ê¸°ì¤€):")
            for i, row in enumerate(agent_data_sorted.itertuples(), 1):
                print(f"    {i}. {row.provider_model}: F1 {row.f1_score:.3f} ({int(row.test_count)}ê°œ í…ŒìŠ¤íŠ¸)")

        print("\n" + "=" * 80)

def main():
    parser = argparse.ArgumentParser(description='ì—ì´ì „íŠ¸ë³„ ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™”')
    parser.add_argument('results_file', help='ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ íŒŒì¼ (JSON)')
    parser.add_argument('--min-tests', type=int, default=10, help='ìµœì†Œ í…ŒìŠ¤íŠ¸ ìˆ˜ (ê¸°ë³¸ê°’: 10)')
    parser.add_argument('--heatmap', action='store_true', help='íˆíŠ¸ë§µ ìƒì„±')
    parser.add_argument('--bar', action='store_true', help='ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±')
    parser.add_argument('--ranking', action='store_true', help='ìˆœìœ„ ì°¨íŠ¸ ìƒì„±')
    parser.add_argument('--comprehensive', action='store_true', help='ì¢…í•© ë¹„êµ ê·¸ë˜í”„ ìƒì„±')
    parser.add_argument('--summary', action='store_true', help='ìš”ì•½ ì¶œë ¥')
    parser.add_argument('--all', action='store_true', help='ëª¨ë“  ê·¸ë˜í”„ ë° ìš”ì•½ ìƒì„±')

    args = parser.parse_args()

    if not Path(args.results_file).exists():
        print(f"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.results_file}")
        return

    visualizer = AgentPerformanceVisualizer(args.results_file)

    if args.all or args.summary:
        visualizer.print_best_models_summary(args.min_tests)

    if args.all or args.heatmap:
        print("\nğŸ“Š íˆíŠ¸ë§µ ìƒì„± ì¤‘...")
        visualizer.create_agent_model_heatmap('f1_score', args.min_tests)

    if args.all or args.bar:
        print("\nğŸ“Š ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        visualizer.create_agent_comparison_bar(args.min_tests)

    if args.all or args.ranking:
        print("\nğŸ“Š ìˆœìœ„ ì°¨íŠ¸ ìƒì„± ì¤‘...")
        visualizer.create_agent_model_ranking(args.min_tests)

    if args.all or args.comprehensive:
        print("\nğŸ“Š ì¢…í•© ë¹„êµ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        visualizer.create_comprehensive_comparison(args.min_tests)

    print("\nâœ… ì™„ë£Œ!")

if __name__ == "__main__":
    main()
