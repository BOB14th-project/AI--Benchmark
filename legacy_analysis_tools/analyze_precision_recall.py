#!/usr/bin/env python3
"""
Precision, Recall, F1 Score ì •í™•ë„ ë¶„ì„ ë„êµ¬
ê±°ì§“ ì–‘ì„±(False Positive)ê³¼ ê±°ì§“ ìŒì„±(False Negative)ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ë¶„ì„
"""

import json
import argparse
from typing import Dict, List, Any
from collections import defaultdict


class PrecisionRecallAnalyzer:
    """ì •ë°€ë„/ì¬í˜„ìœ¨ ë¶„ì„ê¸°"""

    ALGORITHM_VARIATIONS = {
        'rsa': ['rsa'],
        'ecc': ['ecc', 'ecdsa', 'ecdh', 'elliptic curve', 'p-256', 'secp256'],
        'aes': ['aes', 'rijndael', 'aes-128', 'aes-256'],
        'md5': ['md5', 'message digest 5'],
        'seed': ['seed'],
        'aria': ['aria'],
        'hight': ['hight'],
        'lea': ['lea'],
        'sha1': ['sha1', 'sha-1'],
        'sha256': ['sha256', 'sha-256'],
        '3des': ['3des', 'triple des', 'triple-des', 'tdes'],
        'des': ['des'],
        'rc4': ['rc4', 'arcfour'],
        'dsa': ['dsa'],
        'dh': ['diffie-hellman', 'dh', 'diffie hellman'],
    }

    def __init__(self, results_file: str):
        self.results_file = results_file
        self.results = None

    def load_results(self) -> bool:
        """ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.results_file, 'r', encoding='utf-8') as f:
                self.results = json.load(f)
            print(f"âœ… ê²°ê³¼ íŒŒì¼ ë¡œë“œ: {self.results_file}\n")
            return True
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def normalize_algorithm_name(self, algorithm: str) -> str:
        """ì•Œê³ ë¦¬ì¦˜ ì´ë¦„ì„ í‘œì¤€í™”"""
        algorithm_lower = algorithm.lower()

        for standard_name, variations in self.ALGORITHM_VARIATIONS.items():
            if any(var in algorithm_lower for var in variations):
                return standard_name.upper()

        return algorithm.upper()

    def extract_detected_algorithms(self, analysis_results: Dict[str, Any]) -> set:
        """ë¶„ì„ ê²°ê³¼ì—ì„œ íƒì§€ëœ ì•Œê³ ë¦¬ì¦˜ ì¶”ì¶œ"""
        detected = set()
        analysis_text = json.dumps(analysis_results).lower()

        for standard_name, variations in self.ALGORITHM_VARIATIONS.items():
            for variation in variations:
                if variation in analysis_text:
                    detected.add(standard_name.upper())
                    break

        return detected

    def calculate_metrics_for_test(self, test_result: Dict[str, Any], ground_truth: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ì— ëŒ€í•œ ì •ë°€ë„/ì¬í˜„ìœ¨ ê³„ì‚°"""

        # ì˜ˆìƒ ì•Œê³ ë¦¬ì¦˜
        expected_algorithms = set()
        for algo in ground_truth.get('expected_findings', {}).get('vulnerable_algorithms_detected', []):
            normalized = self.normalize_algorithm_name(algo)
            expected_algorithms.add(normalized)

        # í•œêµ­ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€
        for algo in ground_truth.get('expected_findings', {}).get('korean_algorithms_detected', []):
            normalized = self.normalize_algorithm_name(algo)
            expected_algorithms.add(normalized)

        # íƒì§€ëœ ì•Œê³ ë¦¬ì¦˜
        detected_algorithms = self.extract_detected_algorithms(
            test_result.get('analysis_results', {})
        )

        # TP, FP, FN ê³„ì‚°
        true_positives = expected_algorithms & detected_algorithms
        false_positives = detected_algorithms - expected_algorithms
        false_negatives = expected_algorithms - detected_algorithms

        tp_count = len(true_positives)
        fp_count = len(false_positives)
        fn_count = len(false_negatives)

        # Precision, Recall, F1 ê³„ì‚°
        precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0.0
        recall = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0.0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

        return {
            'expected_algorithms': sorted(list(expected_algorithms)),
            'detected_algorithms': sorted(list(detected_algorithms)),
            'true_positives': sorted(list(true_positives)),
            'false_positives': sorted(list(false_positives)),
            'false_negatives': sorted(list(false_negatives)),
            'tp_count': tp_count,
            'fp_count': fp_count,
            'fn_count': fn_count,
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score
        }

    def analyze_by_model(self) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ Precision/Recall/F1 ë¶„ì„"""
        if not self.results:
            return {}

        model_metrics = defaultdict(lambda: {
            'tests': [],
            'total_tp': 0,
            'total_fp': 0,
            'total_fn': 0
        })

        # Ground truth ë¡œë“œ
        ground_truths = {}
        for test in self.results.get('detailed_results', []):
            test_id = test.get('test_id')
            if test_id and test_id not in ground_truths:
                gt_path = f"data/ground_truth/source_code/{test_id}.json"
                try:
                    with open(gt_path, 'r', encoding='utf-8') as f:
                        ground_truths[test_id] = json.load(f)
                except:
                    pass

        # ëª¨ë¸ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
        for test in self.results.get('detailed_results', []):
            if not test.get('success'):
                continue

            model = test.get('model', 'unknown')
            test_id = test.get('test_id')

            if test_id not in ground_truths:
                continue

            metrics = self.calculate_metrics_for_test(test, ground_truths[test_id])

            model_metrics[model]['tests'].append(metrics)
            model_metrics[model]['total_tp'] += metrics['tp_count']
            model_metrics[model]['total_fp'] += metrics['fp_count']
            model_metrics[model]['total_fn'] += metrics['fn_count']

        # í‰ê·  ê³„ì‚°
        results = {}
        for model, data in model_metrics.items():
            tests = data['tests']
            if not tests:
                continue

            avg_precision = sum(t['precision'] for t in tests) / len(tests)
            avg_recall = sum(t['recall'] for t in tests) / len(tests)
            avg_f1 = sum(t['f1_score'] for t in tests) / len(tests)

            # Micro-averaged metrics (ì „ì²´ TP, FP, FN ê¸°ì¤€)
            total_tp = data['total_tp']
            total_fp = data['total_fp']
            total_fn = data['total_fn']

            micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
            micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
            micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0.0

            results[model] = {
                'test_count': len(tests),
                'macro_avg': {
                    'precision': avg_precision,
                    'recall': avg_recall,
                    'f1_score': avg_f1
                },
                'micro_avg': {
                    'precision': micro_precision,
                    'recall': micro_recall,
                    'f1_score': micro_f1
                },
                'totals': {
                    'tp': total_tp,
                    'fp': total_fp,
                    'fn': total_fn
                },
                'detailed_tests': tests
            }

        return results

    def print_analysis(self):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        model_metrics = self.analyze_by_model()

        if not model_metrics:
            print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print("=" * 100)
        print("ğŸ“Š Precision, Recall, F1 Score ì •ë°€ ë¶„ì„")
        print("=" * 100)
        print()

        # ëª¨ë¸ë³„ ê²°ê³¼ë¥¼ F1 ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_models = sorted(
            model_metrics.items(),
            key=lambda x: x[1]['micro_avg']['f1_score'],
            reverse=True
        )

        for rank, (model, metrics) in enumerate(sorted_models, 1):
            print(f"{'ğŸ¥‡' if rank == 1 else 'ğŸ¥ˆ' if rank == 2 else 'ğŸ¥‰' if rank == 3 else 'ğŸ“Š'} ëª¨ë¸: {model}")
            print(f"   í…ŒìŠ¤íŠ¸ ìˆ˜: {metrics['test_count']}ê°œ")
            print()

            # Macro-averaged (í…ŒìŠ¤íŠ¸ë³„ í‰ê· )
            print(f"   ğŸ“ˆ Macro-Averaged (í…ŒìŠ¤íŠ¸ë³„ í‰ê· ):")
            print(f"      Precision: {metrics['macro_avg']['precision']:.3f} (ì •ë°€ë„)")
            print(f"      Recall:    {metrics['macro_avg']['recall']:.3f} (ì¬í˜„ìœ¨)")
            print(f"      F1 Score:  {metrics['macro_avg']['f1_score']:.3f}")
            print()

            # Micro-averaged (ì „ì²´ TP/FP/FN ê¸°ì¤€)
            print(f"   ğŸ“Š Micro-Averaged (ì „ì²´ ì•Œê³ ë¦¬ì¦˜ ê¸°ì¤€):")
            print(f"      Precision: {metrics['micro_avg']['precision']:.3f}")
            print(f"      Recall:    {metrics['micro_avg']['recall']:.3f}")
            print(f"      F1 Score:  {metrics['micro_avg']['f1_score']:.3f}")
            print()

            # TP, FP, FN í†µê³„
            print(f"   ğŸ” íƒì§€ í†µê³„:")
            print(f"      âœ… True Positives:  {metrics['totals']['tp']}ê°œ (ì •í™•íˆ íƒì§€)")
            print(f"      âš ï¸  False Positives: {metrics['totals']['fp']}ê°œ (ì˜ëª» íƒì§€)")
            print(f"      âŒ False Negatives: {metrics['totals']['fn']}ê°œ (ë†“ì¹œ íƒì§€)")
            print()

            # ìƒì„¸ í…ŒìŠ¤íŠ¸ë³„ ê²°ê³¼
            print(f"   ğŸ“‹ í…ŒìŠ¤íŠ¸ë³„ ìƒì„¸:")
            for i, test in enumerate(metrics['detailed_tests'], 1):
                print(f"      í…ŒìŠ¤íŠ¸ {i}:")
                print(f"         ì˜ˆìƒ: {', '.join(test['expected_algorithms']) if test['expected_algorithms'] else 'ì—†ìŒ'}")
                print(f"         íƒì§€: {', '.join(test['detected_algorithms']) if test['detected_algorithms'] else 'ì—†ìŒ'}")
                print(f"         âœ… TP: {', '.join(test['true_positives']) if test['true_positives'] else 'ì—†ìŒ'}")
                if test['false_positives']:
                    print(f"         âš ï¸  FP: {', '.join(test['false_positives'])}")
                if test['false_negatives']:
                    print(f"         âŒ FN: {', '.join(test['false_negatives'])}")
                print(f"         F1: {test['f1_score']:.3f}")
                print()

            print("-" * 100)
            print()

        # ì¢…í•© ìˆœìœ„ í…Œì´ë¸”
        print("=" * 100)
        print("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (F1 Score ê¸°ì¤€)")
        print("=" * 100)
        print(f"{'ìˆœìœ„':<6} {'ëª¨ë¸':<25} {'Precision':<12} {'Recall':<12} {'F1 Score':<12} {'TP':<8} {'FP':<8} {'FN':<8}")
        print("-" * 100)

        for rank, (model, metrics) in enumerate(sorted_models, 1):
            m = metrics['micro_avg']
            t = metrics['totals']
            medal = 'ğŸ¥‡' if rank == 1 else 'ğŸ¥ˆ' if rank == 2 else 'ğŸ¥‰' if rank == 3 else '  '
            print(f"{medal} {rank:<3} {model:<25} {m['precision']:<12.3f} {m['recall']:<12.3f} {m['f1_score']:<12.3f} {t['tp']:<8} {t['fp']:<8} {t['fn']:<8}")

        print("=" * 100)


def main():
    parser = argparse.ArgumentParser(description='Precision, Recall, F1 Score ë¶„ì„')
    parser.add_argument('--file', type=str, help='ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ JSON íŒŒì¼')

    args = parser.parse_args()

    # íŒŒì¼ ì§€ì •
    results_file = args.file if args.file else 'benchmark_results_1760143167.json'

    # ë¶„ì„ ì‹¤í–‰
    analyzer = PrecisionRecallAnalyzer(results_file)

    if analyzer.load_results():
        analyzer.print_analysis()


if __name__ == '__main__':
    main()
