{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Llama 3.1 vs PQC-tuned Llama Benchmark (Colab)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Base Llama 3.1 ëª¨ë¸ê³¼ PQC-tuned Llama ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "**í•„ìš” ì‚¬í•­:**\n",
    "- Google Colab GPU (T4 ì´ìƒ ê¶Œì¥)\n",
    "- Hugging Face í† í° (Meta Llama 3.1 ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ í•„ìš”)\n",
    "\n",
    "**ì‹¤í–‰ ì‹œê°„:** ì•½ 2-3ì‹œê°„ (79ê°œ í…ŒìŠ¤íŠ¸ íŒŒì¼ Ã— 2ê°œ ëª¨ë¸)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ í™•ì¸ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™•ì¸\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜„ì¬ ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "import os\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì €ì¥ì†Œ í´ë¡  ë° ì´ë™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub ì €ì¥ì†Œ í´ë¡ \n",
    "!git clone https://github.com/BOB14th-project/AI--Benchmark.git\n",
    "%cd AI--Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "ì•½ 5-10ë¶„ ì†Œìš”ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (8-bit quantizationì„ ìœ„í•´ bitsandbytes ì¶”ê°€)\n!pip install -q transformers peft torch tqdm huggingface_hub accelerate bitsandbytes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flash Attention 2 ì„¤ì¹˜ (ì„ íƒì‚¬í•­ - ì†ë„ í–¥ìƒ)\n",
    "# ì„¤ì¹˜ì— ì‹¤íŒ¨í•´ë„ ë²¤ì¹˜ë§ˆí¬ëŠ” ì‹¤í–‰ë©ë‹ˆë‹¤ (ì•½ê°„ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ)\n",
    "!pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hugging Face í† í° ì„¤ì •\n",
    "\n",
    "**ì¤‘ìš”:** ì•„ë˜ ì…€ì—ì„œ `YOUR_HF_TOKEN_HERE`ë¥¼ ì‹¤ì œ Hugging Face í† í°ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”.\n",
    "\n",
    "í† í° ë°œê¸‰ ë°©ë²•:\n",
    "1. https://huggingface.co/settings/tokens ì ‘ì†\n",
    "2. \"New token\" í´ë¦­\n",
    "3. Read ê¶Œí•œìœ¼ë¡œ í† í° ìƒì„±\n",
    "4. Meta Llama 3.1 ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ ì‹ ì²­: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ì—¬ê¸°ì— Hugging Face í† í°ì„ ì…ë ¥í•˜ì„¸ìš”\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # ì˜ˆ: \"hf_xxxxxxxxxxxxx\"\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "print(\"âœ… Hugging Face í† í° ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í…ŒìŠ¤íŠ¸ íŒŒì¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ íŒŒì¼ ë° Ground truth í™•ì¸\n",
    "!echo \"=== í…ŒìŠ¤íŠ¸ íŒŒì¼ ===\"\n",
    "!ls data/test_files/assembly_binary/*.s | wc -l\n",
    "!echo \"\"\n",
    "!echo \"=== Ground truth íŒŒì¼ ===\"\n",
    "!ls data/ground_truth/assembly_binary/*.json | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë²¤ì¹˜ë§ˆí¬ ì„¤ì •\n",
    "\n",
    "í…ŒìŠ¤íŠ¸ ê°œìˆ˜ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "- `TEST_LIMIT = None`: ì „ì²´ í…ŒìŠ¤íŠ¸ (79ê°œ, ì•½ 2-3ì‹œê°„)\n",
    "- `TEST_LIMIT = 10`: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10ê°œ, ì•½ 20-30ë¶„)\n",
    "- `TEST_LIMIT = 3`: ë§¤ìš° ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (3ê°œ, ì•½ 5-10ë¶„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¤ì¹˜ë§ˆí¬ ì„¤ì • ìˆ˜ì • (ì„ íƒì‚¬í•­)\n",
    "# TEST_LIMIT ê°’ì„ ë³€ê²½í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”\n",
    "\n",
    "# import re\n",
    "\n",
    "# # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10ê°œ íŒŒì¼ë§Œ)\n",
    "# !sed -i 's/TEST_LIMIT = None/TEST_LIMIT = 10/' benchmark_base_vs_tuned_binary.py\n",
    "\n",
    "# # ë˜ëŠ” ì „ì²´ í…ŒìŠ¤íŠ¸ (ê¸°ë³¸ê°’)\n",
    "# # !sed -i 's/TEST_LIMIT = [0-9]*/TEST_LIMIT = None/' benchmark_base_vs_tuned_binary.py\n",
    "\n",
    "# í˜„ì¬ ì„¤ì • í™•ì¸\n",
    "!grep \"TEST_LIMIT =\" benchmark_base_vs_tuned_binary.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰\n",
    "\n",
    "**ê²½ê³ :** ì´ ì…€ì€ ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤!\n",
    "- ì „ì²´ í…ŒìŠ¤íŠ¸: ì•½ 2-3ì‹œê°„\n",
    "- 10ê°œ í…ŒìŠ¤íŠ¸: ì•½ 20-30ë¶„\n",
    "\n",
    "ì‹¤í–‰ ì¤‘ì— Colab íƒ­ì„ ë‹«ì•„ë„ ë˜ì§€ë§Œ, ì»´í“¨í„°ë¥¼ ì ˆì „ ëª¨ë“œë¡œ ë‘ë©´ ì—°ê²°ì´ ëŠì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰\n",
    "!python3 benchmark_base_vs_tuned_binary.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ íŒŒì¼ í™•ì¸\n",
    "!ls -lh results/base_vs_tuned_binary_*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì‹  ê²°ê³¼ íŒŒì¼ ë‚´ìš© í™•ì¸\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# ê°€ì¥ ìµœê·¼ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°\n",
    "result_files = sorted(glob.glob(\"results/base_vs_tuned_binary_*.json\"), reverse=True)\n",
    "\n",
    "if result_files:\n",
    "    latest_result = result_files[0]\n",
    "    print(f\"ğŸ“Š ìµœì‹  ê²°ê³¼ íŒŒì¼: {latest_result}\\n\")\n",
    "    \n",
    "    with open(latest_result, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # ìš”ì•½ ì •ë³´ ì¶œë ¥\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ë²¤ì¹˜ë§ˆí¬ ì •ë³´\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ì‹¤í–‰ ì‹œê°„: {data['benchmark_info']['timestamp']}\")\n",
    "    print(f\"ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {data['benchmark_info']['total_tests']}\")\n",
    "    print(f\"Base ëª¨ë¸: {data['benchmark_info']['base_model']}\")\n",
    "    print(f\"Tuned ëª¨ë¸: {data['benchmark_info']['tuned_model']}\")\n",
    "    \n",
    "    # Base ëª¨ë¸ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    base_results = [r for r in data['base_results'] if 'error' not in r]\n",
    "    if base_results:\n",
    "        base_tp = sum(r['true_positives'] for r in base_results)\n",
    "        base_fp = sum(r['false_positives'] for r in base_results)\n",
    "        base_fn = sum(r['false_negatives'] for r in base_results)\n",
    "        base_precision = base_tp / (base_tp + base_fp) if (base_tp + base_fp) > 0 else 0\n",
    "        base_recall = base_tp / (base_tp + base_fn) if (base_tp + base_fn) > 0 else 0\n",
    "        base_f1 = 2 * base_precision * base_recall / (base_precision + base_recall) if (base_precision + base_recall) > 0 else 0\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Base Llama 3.1 ê²°ê³¼\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Precision: {base_precision:.3f}\")\n",
    "        print(f\"Recall: {base_recall:.3f}\")\n",
    "        print(f\"F1-Score: {base_f1:.3f}\")\n",
    "        print(f\"TP: {base_tp}, FP: {base_fp}, FN: {base_fn}\")\n",
    "    \n",
    "    # Tuned ëª¨ë¸ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    tuned_results = [r for r in data['tuned_results'] if 'error' not in r]\n",
    "    if tuned_results:\n",
    "        tuned_tp = sum(r['true_positives'] for r in tuned_results)\n",
    "        tuned_fp = sum(r['false_positives'] for r in tuned_results)\n",
    "        tuned_fn = sum(r['false_negatives'] for r in tuned_results)\n",
    "        tuned_precision = tuned_tp / (tuned_tp + tuned_fp) if (tuned_tp + tuned_fp) > 0 else 0\n",
    "        tuned_recall = tuned_tp / (tuned_tp + tuned_fn) if (tuned_tp + tuned_fn) > 0 else 0\n",
    "        tuned_f1 = 2 * tuned_precision * tuned_recall / (tuned_precision + tuned_recall) if (tuned_precision + tuned_recall) > 0 else 0\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"PQC-tuned Llama ê²°ê³¼\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Precision: {tuned_precision:.3f}\")\n",
    "        print(f\"Recall: {tuned_recall:.3f}\")\n",
    "        print(f\"F1-Score: {tuned_f1:.3f}\")\n",
    "        print(f\"TP: {tuned_tp}, FP: {tuned_fp}, FN: {tuned_fn}\")\n",
    "        \n",
    "        # ê°œì„ ìœ¨ ê³„ì‚°\n",
    "        if base_f1 > 0:\n",
    "            improvement = ((tuned_f1 - base_f1) / base_f1) * 100\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ê°œì„ ìœ¨\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"F1-Score ê°œì„ : {improvement:+.2f}%\")\n",
    "else:\n",
    "    print(\"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ê²°ê³¼ ë‹¤ìš´ë¡œë“œ\n",
    "\n",
    "ê²°ê³¼ íŒŒì¼ì„ ë¡œì»¬ ì»´í“¨í„°ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# ê°€ì¥ ìµœê·¼ ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "result_files = sorted(glob.glob(\"results/base_vs_tuned_binary_*.json\"), reverse=True)\n",
    "\n",
    "if result_files:\n",
    "    latest_result = result_files[0]\n",
    "    print(f\"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {latest_result}\")\n",
    "    files.download(latest_result)\n",
    "    print(\"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(\"âŒ ë‹¤ìš´ë¡œë“œí•  ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ì¶”ê°€ ë¶„ì„ (ì„ íƒì‚¬í•­)\n",
    "\n",
    "ê°œë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë³„ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ í™•ì¸\n",
    "import pandas as pd\n",
    "\n",
    "if result_files:\n",
    "    with open(latest_result, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Base ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    base_df = pd.DataFrame(data['base_results'])\n",
    "    tuned_df = pd.DataFrame(data['tuned_results'])\n",
    "    \n",
    "    print(\"=== Base Model - ìƒìœ„ 10ê°œ ê²°ê³¼ ===\")\n",
    "    if 'test_id' in base_df.columns:\n",
    "        display(base_df[['test_id', 'true_positives', 'false_positives', 'false_negatives', 'response_time']].head(10))\n",
    "    \n",
    "    print(\"\\n=== Tuned Model - ìƒìœ„ 10ê°œ ê²°ê³¼ ===\")\n",
    "    if 'test_id' in tuned_df.columns:\n",
    "        display(tuned_df[['test_id', 'true_positives', 'false_positives', 'false_negatives', 'response_time']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¬¸ì œ í•´ê²°\n",
    "\n",
    "### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±\n",
    "- TEST_LIMITì„ ë” ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”\n",
    "- ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”\n",
    "\n",
    "### Hugging Face í† í° ì˜¤ë¥˜\n",
    "- í† í°ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”\n",
    "- Meta Llama 3.1 ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œì„ ì‹ ì²­í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”\n",
    "\n",
    "### ì—°ê²° ëŠê¹€\n",
    "- Colab Proë¥¼ ì‚¬ìš©í•˜ë©´ ë” ê¸´ ì‹¤í–‰ ì‹œê°„ì„ ë³´ì¥ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "- ì¤‘ê°„ ê²°ê³¼ëŠ” ìë™ìœ¼ë¡œ ì €ì¥ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}