# π€ λ΅μ»¬ AI μ„λ²„ λ²¤μΉλ§ν¬ λΉ λ¥Έ μ‹μ‘ κ°€μ΄λ“

## β΅ λΉ λ¥Έ μ„¤μ • (5λ¶„ μ΄λ‚΄)

### 1λ‹¨κ³„: λ΅μ»¬ AI μ„λ²„ μ •λ³΄ ν™•μΈ

λ¨Όμ € λ΅μ»¬ AI μ„λ²„μ λ‹¤μ μ •λ³΄λ¥Ό ν™•μΈν•μ„Έμ”:

- **μ„λ²„ μ£Όμ†**: μ) `http://localhost:8000`
- **API μ—”λ“ν¬μΈνΈ**: μ) `/v1/chat/completions` (OpenAI νΈν™) λλ” `/generate` (μ»¤μ¤ν…€)
- **λ¨λΈ μ΄λ¦„**: μ) `llama3-rag`, `custom-model`
- **API ν‚¤**: ν•„μ”ν• κ²½μ°

### 2λ‹¨κ³„: ν™κ²½ μ„¤μ •

`.env` νμΌμ„ μ—΄κ³  λ‹¤μ μ„¤μ •μ„ μ¶”κ°€ν•μ„Έμ”:

```bash
# Local AI Server Configuration
LOCAL_AI_API_KEY=not_required
LOCAL_AI_MODEL=your-model-name
LOCAL_AI_BASE_URL=http://localhost:8000/v1
```

**β οΈ μ¤‘μ”**:
- OpenAI νΈν™ APIλ¥Ό μ‚¬μ©ν•λ” κ²½μ° BASE_URL λμ— `/v1`μ„ μ¶”κ°€ν•μ„Έμ”
- μ»¤μ¤ν…€ APIλ¥Ό μ‚¬μ©ν•λ” κ²½μ° `/v1` μ—†μ΄ λ£¨νΈ URLλ§ μ…λ ¥ν•μ„Έμ”

### 3λ‹¨κ³„: μ—°κ²° ν…μ¤νΈ

```bash
python test_local_connection.py
```

λ‹¤μκ³Ό κ°™μ€ μ¶λ ¥μ΄ λ‚μ¤λ©΄ μ„±κ³µμ…λ‹λ‹¤:

```
β… μ„λ²„κ°€ μ‹¤ν–‰ μ¤‘μ…λ‹λ‹¤!
β… μ”μ²­ μ„±κ³µ!
β… λ¨λ“  ν…μ¤νΈ μ™„λ£!
```

### 4λ‹¨κ³„: λ²¤μΉλ§ν¬ μ‹¤ν–‰

```bash
# λ°©λ²• 1: λ΅μ»¬ AIλ§ ν…μ¤νΈ
python test_model.py --provider local_ai

# λ°©λ²• 2: νΉμ • μ—μ΄μ „νΈ ν…μ¤νΈ
python test_agent.py --agent source_code --provider local_ai

# λ°©λ²• 3: μ „μ²΄ λ²¤μΉλ§ν¬ (λ¨λ“  λ¨λΈ ν¬ν•¨)
python run_benchmark.py
```

### 5λ‹¨κ³„: κ²°κ³Ό λ¶„μ„

```bash
python analyze_and_visualize.py benchmark_results.json
```

μƒμ„±λ νμΌ:
- `COMPREHENSIVE_REPORT.txt`: μΆ…ν•© λ³΄κ³ μ„
- `model_f1_comparison.png`: F1 Score λΉ„κµ
- `model_response_time.png`: μ‘λ‹µ μ‹κ°„ λΉ„κµ

---

## π”§ μ»¤μ¤ν…€ API μ‚¬μ© μ‹ μ¶”κ°€ μ„¤μ •

OpenAI νΈν™μ΄ μ•„λ‹ μ»¤μ¤ν…€ APIλ¥Ό μ‚¬μ©ν•λ” κ²½μ°, `clients/local_ai_client.py` νμΌμ„ μμ •ν•΄μ•Ό ν•©λ‹λ‹¤:

### μμ • μ„μΉ 1: μ”μ²­ μ—”λ“ν¬μΈνΈ (96λ²μ§Έ μ¤„ κ·Όμ²)

```python
# μμ‹: μ»¤μ¤ν…€ μ—”λ“ν¬μΈνΈλ΅ λ³€κ²½
endpoint = f"{self.base_url}/api/inference"  # λλ” /generate, /predict λ“±
```

### μμ • μ„μΉ 2: μ”μ²­ λ°μ΄ν„° ν•μ‹ (88-93λ²μ§Έ μ¤„)

```python
# μμ‹: λ΅μ»¬ μ„λ²„μ μ”μ²­ ν•μ‹μ— λ§κ² λ³€κ²½
data = {
    "query": prompt,          # λλ” "text", "input" λ“±
    "max_length": max_tokens, # λλ” "max_tokens", "length" λ“±
    "temperature": 0.1,
    "model": self.model
}
```

### μμ • μ„μΉ 3: μ‘λ‹µ νμ‹± (116-125λ²μ§Έ μ¤„)

```python
# μμ‹: λ΅μ»¬ μ„λ²„μ μ‘λ‹µ ν•μ‹μ— λ§κ² λ³€κ²½
content = response_json.get('generated_text', '')
# λλ”
# content = response_json.get('response', '')
# λλ”
# content = response_json.get('output', {}).get('text', '')
```

---

## π“ RAG μ‹μ¤ν… ν†µν•© ν

RAG μ‹μ¤ν…μ„ κ°–μ¶ λ΅μ»¬ AI μ„λ²„λ¥Ό ν…μ¤νΈν•λ” κ²½μ°:

### 1. RAG μ§€μ‹ λ² μ΄μ¤ μ¤€λΉ„

λ²¤μΉλ§ν¬ μ‹¤ν–‰ μ „μ— λ‹¤μ νμΌλ“¤μ„ λ΅μ»¬ AI μ„λ²„μ— λ΅λ“ν•μ„Έμ”:

```bash
data/rag_knowledge/source_code_agent_reference.json
data/rag_knowledge/assembly_binary_agent_reference.json
data/rag_knowledge/logs_config_agent_reference.json
```

### 2. RAG μ„±λ¥ μΈ΅μ •

RAGκ°€ μλ” λ¨λΈκ³Ό μ—†λ” λ¨λΈμ„ λΉ„κµ:

```bash
# RAG μ—†λ” λ¨λΈ (κΈ°λ³Έ Ollama)
python test_model.py --provider ollama --model llama3:8b

# RAG μλ” λ¨λΈ (λ΅μ»¬ AI)
python test_model.py --provider local_ai

# κ²°κ³Ό λΉ„κµ
python analyze_and_visualize.py benchmark_results.json
```

### 3. νƒ€μ„μ•„μ›ƒ μ΅°μ •

RAG μ²λ¦¬ μ‹κ°„μ„ κ³ λ ¤ν•μ—¬ `config/config.yaml`μ νƒ€μ„μ•„μ›ƒμ„ λλ¦¬μ„Έμ”:

```yaml
benchmark:
  timeout_seconds: 90  # RAG μ‹μ¤ν…μ©μΌλ΅ μ¦κ°€
```

---

## π› λ¬Έμ  ν•΄κ²°

### μ—°κ²° μ¤λ¥

```
Error: Connection refused
```

**ν•΄κ²°**:
1. λ΅μ»¬ AI μ„λ²„κ°€ μ‹¤ν–‰ μ¤‘μΈμ§€ ν™•μΈ
2. ν¬νΈ λ²νΈ ν™•μΈ
   ```bash
   curl http://localhost:8000/health
   ```

### λ¨λΈμ„ μ°Ύμ„ μ μ—†μ

```
Error: Model 'your-model' not found
```

**ν•΄κ²°**:
1. μ‚¬μ© κ°€λ¥ν• λ¨λΈ ν™•μΈ
   ```bash
   curl http://localhost:8000/v1/models
   ```
2. `.env` νμΌμ `LOCAL_AI_MODEL` μμ •

### νƒ€μ„μ•„μ›ƒ

```
Error: Request timeout
```

**ν•΄κ²°**:
1. `config/config.yaml`μ `timeout_seconds` μ¦κ°€
2. λ΅μ»¬ μ„λ²„ λ¦¬μ†μ¤ ν™•μΈ (GPU λ©”λ¨λ¦¬, CPU)

### JSON νμ‹± μ¤λ¥

```
Error: Invalid JSON response
```

**ν•΄κ²°**:
1. λ΅μ»¬ AI μ„λ²„κ°€ JSON ν•μ‹μΌλ΅ μ‘λ‹µν•λ”μ§€ ν™•μΈ
2. `clients/local_ai_client.py`μ μ‘λ‹µ νμ‹± λ΅μ§ μμ •
3. ν”„λ΅¬ν”„νΈμ— JSON ν•μ‹ λ…μ‹μ  μ”μ²­:
   ```python
   prompt = "... Please respond in JSON format: {...}"
   ```

---

## π“ μμƒ κ²°κ³Ό

λ²¤μΉλ§ν¬ μ‹¤ν–‰ ν›„ λ‹¤μ ν•­λ©λ“¤μ΄ ν‰κ°€λ©λ‹λ‹¤:

1. **F1 Score**: μ–‘μ μ·¨μ•½ μ•κ³ λ¦¬μ¦ νƒμ§€ μ •ν™•λ„
2. **Precision**: νƒμ§€λ κ²ƒ μ¤‘ μ‹¤μ  μ·¨μ•½ μ•κ³ λ¦¬μ¦ λΉ„μ¨
3. **Recall**: μ‹¤μ  μ·¨μ•½ μ•κ³ λ¦¬μ¦ μ¤‘ νƒμ§€λ λΉ„μ¨
4. **Response Time**: ν‰κ·  μ‘λ‹µ μ‹κ°„
5. **Algorithm Detection Rate**: μ•κ³ λ¦¬μ¦λ³„ νƒμ§€μ¨

### RAG μ‹μ¤ν…μ κΈ°λ€ ν¨κ³Ό

- β… **Recall μ¦κ°€**: λ„λ½λ μ•κ³ λ¦¬μ¦ νƒμ§€ κ°μ„ 
- β… **ν•κµ­ μ•κ³ λ¦¬μ¦ νƒμ§€**: SEED, ARIA, KCDSA λ“± νƒμ§€μ¨ ν–¥μƒ
- β… **Precision μ μ§€/μ¦κ°€**: μ¤νƒ κ°μ†
- β οΈ **Response Time μ¦κ°€**: RAG κ²€μƒ‰μΌλ΅ μΈν• μ•½κ°„μ μ†λ„ μ €ν• (ν—μ© λ²”μ„)

---

## π“ μ¶”κ°€ λ¦¬μ†μ¤

### μƒμ„Έ κ°€μ΄λ“
- [LOCAL_AI_SERVER_INTEGRATION.md](docs/LOCAL_AI_SERVER_INTEGRATION.md)

### μƒμ„±λ νμΌλ“¤
- `clients/local_ai_client.py`: λ΅μ»¬ AI ν΄λΌμ΄μ–ΈνΈ
- `test_local_connection.py`: μ—°κ²° ν…μ¤νΈ μ¤ν¬λ¦½νΈ
- `setup_local_ai.py`: μλ™ μ„¤μ • μ¤ν¬λ¦½νΈ

### λ²¤μΉλ§ν¬ μ»¤μ¤ν„°λ§μ΄μ§•
- `config/config.yaml`: λ²¤μΉλ§ν¬ μ„¤μ •
- `agents/`: μ—μ΄μ „νΈ ν”„λ΅¬ν”„νΈ μμ •
- `utils/metrics_calculator.py`: ν‰κ°€ μ§€ν‘ μ¶”κ°€

---

## π’΅ μλ™ μ„¤μ • λ„κµ¬

μλ™ μ„¤μ •μ΄ λ²κ±°λ΅μ΄ κ²½μ°, μλ™ μ„¤μ • μ¤ν¬λ¦½νΈλ¥Ό μ‚¬μ©ν•μ„Έμ”:

```bash
python setup_local_ai.py --interactive
```

λ€ν™”ν• λ¨λ“λ΅ λ¨λ“  μ„¤μ •μ„ μλ™μΌλ΅ μ™„λ£ν•©λ‹λ‹¤.

---

## π― λ‹¤μ λ‹¨κ³„

1. β… λ΅μ»¬ AI μ„λ²„ μ—°κ²° ν…μ¤νΈ
2. β… λ‹¨μΌ νμΌλ΅ κΈ°λ³Έ νƒμ§€ ν…μ¤νΈ
3. β… μ—μ΄μ „νΈλ³„ μ„±λ¥ ν‰κ°€
4. β… μ „μ²΄ λ²¤μΉλ§ν¬ μ‹¤ν–‰
5. β… RAG vs Non-RAG μ„±λ¥ λΉ„κµ
6. β… κ²°κ³Ό λ¶„μ„ λ° λ¨λΈ κ°μ„ 

---

κ¶κΈν• μ μ΄ μμΌλ©΄ μ–Έμ λ“  μ§λ¬Έν•μ„Έμ”!

**μ°Έκ³ **: μ΄ λ²¤μΉλ§ν¬λ” μ–‘μ μ»΄ν“¨ν… μ‹λ€λ¥Ό λ€λΉ„ν• μ•”νΈ μ‹μ¤ν…μ **μ–‘μ μ·¨μ•½ μ•κ³ λ¦¬μ¦**μ„ νƒμ§€ν•λ” AI λ¨λΈμ μ„±λ¥μ„ ν‰κ°€ν•©λ‹λ‹¤.
